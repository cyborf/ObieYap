{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xFCx6jZU3m11"
   },
   "source": [
    "<!-- Banner Image -->\n",
    "<img src=\"https://uohmivykqgnnbiouffke.supabase.co/storage/v1/object/public/landingpage/brevdevnotebooks.png\" width=\"100%\">\n",
    "\n",
    "<!-- Links -->\n",
    "<center>\n",
    "  <a href=\"https://console.brev.dev\" style=\"color: #06b6d4;\">Console</a> â€¢\n",
    "  <a href=\"https://brev.dev\" style=\"color: #06b6d4;\">Docs</a> â€¢\n",
    "  <a href=\"/\" style=\"color: #06b6d4;\">Templates</a> â€¢\n",
    "  <a href=\"https://discord.gg/NVDyv7TUgJ\" style=\"color: #06b6d4;\">Discord</a>\n",
    "</center>\n",
    "\n",
    "# Fine-tuning Mistral on your own data ðŸ¤™\n",
    "\n",
    "Welcome!\n",
    "\n",
    "In this notebook and tutorial, we will fine-tune the [Mistral 7B](https://github.com/mistralai/mistral-src) model - which outperforms Llama 2 13B on all tested benchmarks - ***on your own data!***\n",
    "\n",
    "## Watch the accompanying video walk-through [here](https://youtu.be/kmkcNVvEz-k?si=Ogt1wRFNqYI6zXfw&t=1)!\n",
    "\n",
    "I did this for **just one dollar ($1)** on an 1x A10G 24GB from Brev.dev (instructions below).\n",
    "\n",
    "This tutorial will use QLoRA, a fine-tuning method that combines quantization and LoRA. For more information about what those are and how they work, see [this post](https://brev.dev/blog/how-qlora-works).\n",
    "\n",
    "In this notebook, we will load the large model in 4bit using `bitsandbytes` and use LoRA to train using the PEFT library from Hugging Face ðŸ¤—.\n",
    "\n",
    "Note that if you ever have trouble importing something from Huggingface, you may need to run `huggingface-cli login` in a shell. To open a shell in Jupyter Lab, click on 'Launcher' (or the '+' if it's not there) next to the notebook tab at the top of the screen. Under \"Other\", click \"Terminal\" and then run the command.\n",
    "\n",
    "### Help us make this tutorial better! Please provide feedback on the [Discord channel](https://discord.gg/RN2a436M73) or on [X](https://x.com/harperscarroll)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G9TytWkb3m15"
   },
   "source": [
    "#### Before we begin: A note on OOM errors\n",
    "\n",
    "If you get an error like this: `OutOfMemoryError: CUDA out of memory`, tweak your parameters to make the model less computationally intensive. I will help guide you through that in this guide, and if you have any additional questions you can reach out on the [Discord channel](https://discord.gg/RN2a436M73) or on [X](https://x.com/harperscarroll).\n",
    "\n",
    "To re-try after you tweak your parameters, open a Terminal ('Launcher' or '+' in the nav bar above -> Other -> Terminal) and run the command `nvidia-smi`. Then find the process ID `PID` under `Processes` and run the command `kill [PID]`. You will need to re-start your notebook from the beginning. (There may be a better way to do this... if so please do let me know!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VC-9m2yv3m18"
   },
   "source": [
    "## Let's begin!\n",
    "### 0. Preparing data\n",
    "\n",
    "Before you check out a GPU, prepare your dataset for loading and training.\n",
    "\n",
    "To prepare your dataset for loading, all you need are two `.jsonl` files structured something like this:\n",
    "```\n",
    "{\"input\": \"What color is the sky?\", \"output\": \"The sky is blue.\"}\n",
    "{\"input\": \"Where is the best place to get cloud GPUs?\", \"output\": \"Brev.dev\"}\n",
    "```\n",
    "If you choose to model your data as input/output pairs, you'll want to use something like the second `formatting_func` below, which will will combine all your features into one input string.\n",
    "\n",
    "As you can see below, I have `notes.jsonl` for my `train_dataset` and `notes_validation.jsonl` for my `eval_dataset`.\n",
    "\n",
    "I used Exporter, a free local-only app, to export my Apple Notes to `.txt` files, and then I wrote a script to process each note into one `.jsonl` file. Note that for this script, ChatGPT can help out a LOT if you tell it how your data is currently formatted, how you'd like it to be formatted, and ask it to write a script in a certain language you know well (for any debugging) to do so. I also broke up my journal entries so the training sample vector length was smaller (see the discussion on `max_length` and the data visualization below). I broke it into pieces so that contexts were encapsulated entirely, since I did want the model to understand context about my life. My data were ultimately formatted as:\n",
    "\n",
    "```json\n",
    "{\"note\": \"journal-entry-for-model-to-predict\"}\n",
    "{\"note\": \"journal-entry-for-model-to-predict-1\"}\n",
    "{\"note\": \"journal-entry-for-model-to-predict-2\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E2CkxsA43m15"
   },
   "source": [
    "### 1. Instantiate GPU & Load Dataset\n",
    "\n",
    "I used a GPU and dev environment from [brev.dev](https://brev.dev). The whole thing cost me $1 using a 1xA10G 24GB. Click the badge below to get your preconfigured instance:\n",
    "\n",
    "[![](https://uohmivykqgnnbiouffke.supabase.co/storage/v1/object/public/landingpage/brevdeploynavy.svg)](https://console.brev.dev/environment/new?instance=A10G:g5.xlarge&diskStorage=256&name=mistral-finetune-own-data&file=https://github.com/brevdev/notebooks/raw/main/mistral-finetune-own-data.ipynb&python=3.10&cuda=12.0.1)\n",
    "\n",
    "A single A10G (as linked) with 24GB GPU Memory was enough for me. You may need more GPUs and/or Memory if your sequence max_length is larger than 512.\n",
    "\n",
    "Once you've checked out your machine and landed in your instance page, select the specs you'd like (I used **Python 3.10 and CUDA 12.0.1**; these should be preconfigured for you if you use the badge above) and click the \"Build\" button to build your verb container. Give this a few minutes.\n",
    "\n",
    "A few minutes after your model has started Running, click the 'Notebook' button on the top right of your screen once it illuminates (you may need to refresh the screen). You will be taken to a Jupyter Lab environment, where you can upload this Notebook.\n",
    "\n",
    "\n",
    "Note: You can connect your cloud credits (AWS or GCP) by clicking \"Org: \" on the top right, and in the panel that slides over, click \"Connect AWS\" or \"Connect GCP\" under \"Connect your cloud\" and follow the instructions linked to attach your credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "FuXIFTFapAMI",
    "outputId": "c8ced1ad-c7b3-44ba-807b-26d7d13906bc"
   },
   "outputs": [],
   "source": [
    "# You only need to run this once per machine\n",
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "!pip install -q -U datasets scipy ipywidgets matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-9KNTJZkyRgn"
   },
   "source": [
    "\n",
    "Let's use Weights & Biases to track our training metrics. You'll need to apply an API key when prompted. Feel free to skip this if you'd like, and just comment out the `wandb` parameters in the `Trainer` definition below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uhw8JiOr3m18"
   },
   "source": [
    "### Formatting prompts\n",
    "Then create a `formatting_func` to structure training examples as prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_func(example):\n",
    "    text = f\"### Input: {example['input']} ### Output: {example['output']}\"\n",
    "    return text\n",
    "\n",
    "formatting = \"### Input: {example['input']} ### Output: {example['output']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def formatting_func(example):\n",
    "#     text = f\"### The following is a sample event: {example['input']}\"\n",
    "#     return text\n",
    "# formatting = \"### The following is a sample event: {example['input']}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sflV0DL2P64_"
   },
   "source": [
    "Here's another common one:\n",
    "\n",
    "```python\n",
    "def formatting_func(example):\n",
    "    text = f\"### Question: {example['input']}\\n ### Answer: {example['output']}\"\n",
    "    return text\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "shz8Xdv-yRgf"
   },
   "source": [
    "### 2. Load Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJ-5idQwzvg-"
   },
   "source": [
    "Let's now load Mistral - mistralai/Mistral-7B-v0.1 - using 4-bit quantization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /usr/users/quota/students/2020/ymai/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import login\n",
    "login('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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "45524c98039a46d5b7745ad7cb638d2f"
     ]
    },
    "id": "E0Nl5mWL0k2T",
    "outputId": "47b6b01d-e9f2-4b70-919c-17ae64993843"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c62a58b8de54ea8ba34c38b5d04bcb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "base_model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UjNdXolqyRgf"
   },
   "source": [
    "### 3. Tokenization\n",
    "\n",
    "Set up the tokenizer. Add padding on the left as it [makes training use less memory](https://ai.stackexchange.com/questions/41485/while-fine-tuning-a-decoder-only-llm-like-llama-on-chat-dataset-what-kind-of-pa).\n",
    "\n",
    "\n",
    "For `model_max_length`, it's helpful to get a distribution of your data lengths. Let's first tokenize without the truncation/padding, so we can get a length distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "haSUDD9HyRgf",
    "outputId": "22ee95db-2974-4ab0-e0c7-444d04d3e838"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    padding_side=\"left\",\n",
    "    add_eos_token=True,\n",
    "    add_bos_token=True,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def generate_and_tokenize_prompt(prompt):\n",
    "    return tokenizer(formatting_func(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHnKLcq4yRgg"
   },
   "source": [
    "Reformat the prompt and tokenize each sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name = 'chatgpt'\n",
    "train_dataset = load_dataset('json', data_files='{dataset_name}_training.jsonl'.format(dataset_name=dataset_name), split='train')\n",
    "eval_dataset = load_dataset('json', data_files='{dataset_name}_validation.jsonl'.format(dataset_name=dataset_name), split = 'train')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "S3iLAwLh3m19"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a08e2bade29a4ed795e3f30dd8f3bec9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/93 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'formatting_func' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokenized_train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerate_and_tokenize_prompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m tokenized_val_dataset \u001b[38;5;241m=\u001b[39m eval_dataset\u001b[38;5;241m.\u001b[39mmap(generate_and_tokenize_prompt)\n",
      "File \u001b[0;32m~/miniconda3/envs/yap/lib/python3.11/site-packages/datasets/arrow_dataset.py:602\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    600\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    601\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 602\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    603\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[1;32m    605\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/yap/lib/python3.11/site-packages/datasets/arrow_dataset.py:567\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    560\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    565\u001b[0m }\n\u001b[1;32m    566\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 567\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    568\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    569\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/yap/lib/python3.11/site-packages/datasets/arrow_dataset.py:3156\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3151\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[1;32m   3152\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3153\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[1;32m   3154\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3155\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3156\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_single\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   3157\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   3158\u001b[0m \u001b[43m                \u001b[49m\u001b[43mshards_done\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n",
      "File \u001b[0;32m~/miniconda3/envs/yap/lib/python3.11/site-packages/datasets/arrow_dataset.py:3517\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3515\u001b[0m _time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3516\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m shard_iterable:\n\u001b[0;32m-> 3517\u001b[0m     example \u001b[38;5;241m=\u001b[39m \u001b[43mapply_function_on_filtered_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3518\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m update_data:\n\u001b[1;32m   3519\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/yap/lib/python3.11/site-packages/datasets/arrow_dataset.py:3416\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n\u001b[1;32m   3415\u001b[0m     additional_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (rank,)\n\u001b[0;32m-> 3416\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   3418\u001b[0m     processed_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3419\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mkeys_to_format\n\u001b[1;32m   3420\u001b[0m     }\n",
      "Cell \u001b[0;32mIn[4], line 10\u001b[0m, in \u001b[0;36mgenerate_and_tokenize_prompt\u001b[0;34m(prompt)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_and_tokenize_prompt\u001b[39m(prompt):\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer(\u001b[43mformatting_func\u001b[49m(prompt))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'formatting_func' is not defined"
     ]
    }
   ],
   "source": [
    "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt)\n",
    "tokenized_val_dataset = eval_dataset.map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O6ewk27p3m19"
   },
   "source": [
    "Let's get a distribution of our dataset lengths, so we can determine the appropriate `max_length` for our input tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "BA8M9yfC3m19",
    "outputId": "99c6d302-9bb6-47b1-cae9-a1cd870b4770"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset):\n",
    "    lengths = [len(x['input_ids']) for x in tokenized_train_dataset]\n",
    "    lengths += [len(x['input_ids']) for x in tokenized_val_dataset]\n",
    "    print(len(lengths))\n",
    "\n",
    "    # Plotting the histogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(lengths, bins=20, alpha=0.7, color='blue')\n",
    "    plt.xlabel('Length of input_ids')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Lengths of input_ids')\n",
    "    plt.show()\n",
    "\n",
    "plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nBk4Qp_vyRgh"
   },
   "source": [
    "From here, you can choose where you'd like to set the `max_length` to be. You can truncate and pad training examples to fit them to your chosen size. Be aware that choosing a larger `max_length` has its compute tradeoffs.\n",
    "\n",
    "I'm using my personal notes to train the model, and they vary greatly in length. I spent some time cleaning the dataset so the samples were about the same length, cutting up individual notes if needed, but being sure to not cut in the middle of a word or sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bMlw8h743m19"
   },
   "source": [
    "Now let's tokenize again with padding and truncation, and set up the tokenize function to make labels and input_ids the same. This is basically what [self-supervised fine-tuning is](https://neptune.ai/blog/self-supervised-learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "acINaViR3m19"
   },
   "outputs": [],
   "source": [
    "max_length = 200 # This was an appropriate max length for my dataset\n",
    "\n",
    "def generate_and_tokenize_prompt2(prompt):\n",
    "    result = tokenizer(\n",
    "        formatting_func(prompt),\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "518d4f0b89bf4d57bf00d4c6d6e59eb5"
     ]
    },
    "id": "lTk-aTog3m19",
    "outputId": "4fb637b4-77a2-47c6-de7b-4fb620663dd7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49d308922e284b69b315ca3ff592a9dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/60 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3b95ec75bd74b55b3848d87b156641e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt2)\n",
    "tokenized_val_dataset = eval_dataset.map(generate_and_tokenize_prompt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TQL796OayRgh"
   },
   "source": [
    "Check that `input_ids` is padded on the left with the `eos_token` (2) and there is an `eos_token` 2 added to the end, and the prompt starts with a `bos_token` (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "OKHhvxK83m19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 774, 11232, 28747, 19649, 7043, 715, 367, 1986, 774, 15985, 28747, 394, 7173, 16863, 297, 2992, 28725, 1760, 1149, 9023, 28808, 28705, 30012, 243, 162, 143, 162, 19649, 7043, 715, 367, 1986, 15551, 354, 25325, 18185, 10617, 28723, 24443, 354, 1395, 693, 2016, 11787, 16863, 304, 1250, 744, 302, 272, 25325, 27374, 395, 264, 4814, 302, 14130, 25325, 10587, 17163, 28808, 28705, 243, 162, 143, 162, 31057, 243, 162, 149, 191, 2]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_train_dataset[1]['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I6LRa2Zm3m19"
   },
   "source": [
    "Now all the samples should be the same length, `max_length`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "I55Yo3yy3m19",
    "outputId": "c87e344d-e0f3-4542-afcc-4e2025926d64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0kAAAIjCAYAAADWYVDIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAABLLElEQVR4nO3deVhV1f7H8c9BRkFAFBkSkZTEMRXNSG6pYaRmmZTDtVLTbNCcy0vlVBppOVdaVqiZWZZadlNTnG5eNTW1MsVZHAC7FSCWgLB/f/RwfvsIGiBwEN+v59nP7ay9ztrffdhy/bj2XsdiGIYhAAAAAIAkycHeBQAAAABARUJIAgAAAAATQhIAAAAAmBCSAAAAAMCEkAQAAAAAJoQkAAAAADAhJAEAAACACSEJAAAAAEwISQAAAABgQkgCUKlNmDBBFoulXI7Vrl07tWvXzvp606ZNslgs+uyzz8rl+P369VPdunXL5VgllZmZqYEDB8rf318Wi0XDhw+3d0mlrrx/7n9nzZo1at68uVxdXWWxWJSWllZovwULFshisejEiRPlWl9ZKM651K1bV/369SvzmgBcXwhJAK4b+X/xyd9cXV0VGBio6OhozZ49W+fPny+V45w9e1YTJkzQ3r17S2W80lSRayuKV199VQsWLNDTTz+tDz/8UI8++ugV+9atW1f33XdfOVZXPEuWLNHMmTPtXcZV/frrr+rRo4fc3Nz01ltv6cMPP5S7u7u9yyqSn3/+WRMmTKgUoQ3A9cfR3gUAQHG9/PLLCgkJUU5OjlJSUrRp0yYNHz5c06dP15dffqlmzZpZ+7700kv617/+Vazxz549q4kTJ6pu3bpq3rx5kd/3zTffFOs4JXG12ubPn6+8vLwyr+FabNiwQbfffrvGjx9v71Ku2ZIlS/TTTz9V6NmwnTt36vz583rllVcUFRV11b6PPvqoevXqJRcXl3Kq7up+/vlnTZw4Ue3atSv2DGlFOxcA1x9CEoDrTqdOndSqVSvr69jYWG3YsEH33Xef7r//fh04cEBubm6SJEdHRzk6lu2vuj/++ENVq1aVs7NzmR7n7zg5Odn1+EVx7tw5NWrUyN5l3DDOnTsnSfL29v7bvlWqVFGVKlXKuKLyUZnOBYB9cLsdgEqhQ4cOGjt2rE6ePKnFixdb2wt7JmndunWKjIyUt7e3PDw81KBBA73wwguS/nqepHXr1pKk/v37W2/tW7BggaS/njtq0qSJdu/erTvvvFNVq1a1vvfyZ5Ly5ebm6oUXXpC/v7/c3d11//3369SpUzZ9rvRchHnMv6utsGeSLly4oFGjRikoKEguLi5q0KCB3njjDRmGYdPPYrFoyJAhWrlypZo0aSIXFxc1btxYa9asKfwDv8y5c+c0YMAA+fn5ydXVVbfeeqsWLlxo3Z//nM7x48f173//21p7adxKtXjxYoWHh8vNzU0+Pj7q1atXgc83/+f2888/q3379qpatapuuukmTZ06tcB4J0+e1P333y93d3fVqlVLI0aM0Nq1a2WxWLRp0ybreP/+97918uRJ67lc/tnn5eVp8uTJql27tlxdXXX33XfryJEjNn0OHz6smJgY+fv7y9XVVbVr11avXr2Unp7+t+e9bNky63nXrFlTjzzyiM6cOWNzzn379pUktW7dWhaL5arP3hT2HE/+LY/ffvutbrvtNrm6uurmm2/WokWLCn3vli1b9OSTT6pGjRry9PTUY489pt9//92mr8Vi0YQJEwoc3/xnYMGCBXr44YclSe3bt7d+xvmf/98p7FwMw9CkSZNUu3ZtVa1aVe3bt9f+/fsLvDcnJ0cTJ05UaGioXF1dVaNGDUVGRmrdunVFOjaAyoGZJACVxqOPPqoXXnhB33zzjZ544olC++zfv1/33XefmjVrppdfflkuLi46cuSItm7dKklq2LChXn75ZY0bN06DBg3SP/7xD0nSHXfcYR3j119/VadOndSrVy898sgj8vPzu2pdkydPlsVi0ZgxY3Tu3DnNnDlTUVFR2rt3r3XGqyiKUpuZYRi6//77tXHjRg0YMEDNmzfX2rVr9dxzz+nMmTOaMWOGTf9vv/1Wy5cv1zPPPKNq1app9uzZiomJUVJSkmrUqHHFuv7880+1a9dOR44c0ZAhQxQSEqJly5apX79+SktL07Bhw9SwYUN9+OGHGjFihGrXrq1Ro0ZJknx9fYt8/oWZPHmyxo4dqx49emjgwIH65ZdfNGfOHN15553as2ePzQzK77//rnvvvVfdu3dXjx499Nlnn2nMmDFq2rSpOnXqJOmvUNmhQwclJydr2LBh8vf315IlS7Rx40ab47744otKT0/X6dOnrZ+jh4eHTZ/XXntNDg4OGj16tNLT0zV16lT16dNHO3bskCRlZ2crOjpaWVlZevbZZ+Xv768zZ87oq6++Ulpamry8vK543gsWLFD//v3VunVrxcXFKTU1VbNmzdLWrVut5/3iiy+qQYMGevfdd623qNarV6/Yn/GRI0f00EMPacCAAerbt68++OAD9evXT+Hh4WrcuLFN3yFDhsjb21sTJkxQYmKi5s6dq5MnT1pDclHdeeedGjp0qGbPnq0XXnhBDRs2lCTr/5bEuHHjNGnSJHXu3FmdO3fW999/r3vuuUfZ2dk2/SZMmKC4uDgNHDhQt912mzIyMrRr1y59//336tixY4mPD+A6YwDAdSI+Pt6QZOzcufOKfby8vIwWLVpYX48fP94w/6qbMWOGIcn45ZdfrjjGzp07DUlGfHx8gX133XWXIcmYN29eofvuuusu6+uNGzcakoybbrrJyMjIsLZ/+umnhiRj1qxZ1rbg4GCjb9++fzvm1Wrr27evERwcbH29cuVKQ5IxadIkm34PPfSQYbFYjCNHjljbJBnOzs42bfv27TMkGXPmzClwLLOZM2cakozFixdb27Kzs42IiAjDw8PD5tyDg4ONLl26XHW8ovY9ceKEUaVKFWPy5Mk27T/++KPh6Oho057/c1u0aJG1LSsry/D39zdiYmKsbdOmTTMkGStXrrS2/fnnn0ZYWJghydi4caO1vUuXLjafd778n3vDhg2NrKwsa/usWbMMScaPP/5oGIZh7Nmzx5BkLFu27O8/DJPs7GyjVq1aRpMmTYw///zT2v7VV18Zkoxx48ZZ24ryZ+byvsePH7e2BQcHG5KMLVu2WNvOnTtnuLi4GKNGjSrw3vDwcCM7O9vaPnXqVEOS8cUXX1jbJBnjx48vcPzL/wwsW7aswGdeVJefy7lz5wxnZ2ejS5cuRl5enrXfCy+8YEiyOe6tt95a5GsUQOXF7XYAKhUPD4+rrnKXP7PwxRdflHiRAxcXF/Xv37/I/R977DFVq1bN+vqhhx5SQECAvv766xIdv6i+/vprValSRUOHDrVpHzVqlAzD0OrVq23ao6KibGYamjVrJk9PTx07duxvj+Pv76/evXtb25ycnDR06FBlZmZq8+bNpXA2BS1fvlx5eXnq0aOH/ve//1k3f39/hYaGFpj98fDw0COPPGJ97ezsrNtuu83m/NasWaObbrpJ999/v7XN1dX1ijOTV9O/f3+b59TyZ/7yj5c/U7R27Vr98ccfRR53165dOnfunJ555hm5urpa27t06aKwsDD9+9//LnatV9OoUSNr7dJfs38NGjQo9LoYNGiQzbNxTz/9tBwdHcv8Wv8769evV3Z2tp599lmbGa3CFt3w9vbW/v37dfjw4XKsEEBFQ0gCUKlkZmbaBJLL9ezZU23bttXAgQPl5+enXr166dNPPy1WYLrpppuKtUhDaGiozWuLxaL69euX+dLGJ0+eVGBgYIHPI/+WpZMnT9q016lTp8AY1atXL/BMSWHHCQ0NlYOD7f+lXOk4peXw4cMyDEOhoaHy9fW12Q4cOGBdtCBf7dq1C9zydfn5nTx5UvXq1SvQr379+sWu7/LPs3r16pJkPV5ISIhGjhyp9957TzVr1lR0dLTeeuutv30eKf/zbNCgQYF9YWFhpf55F+e6uPxa9/DwUEBAgN2X8c7/TC6vz9fX1/pzyffyyy8rLS1Nt9xyi5o2barnnntOP/zwQ7nVCqBiICQBqDROnz6t9PT0q/6F1s3NTVu2bNH69ev16KOP6ocfflDPnj3VsWNH5ebmFuk4xXmOqKiu9LxGUWsqDVdaDcy4bJGHiiIvL08Wi0Vr1qzRunXrCmzvvPOOTf/yPr+iHG/atGn64Ycf9MILL+jPP//U0KFD1bhxY50+fbpMaiqJ8vrcyvNav5o777xTR48e1QcffKAmTZrovffeU8uWLfXee+/ZuzQA5YiQBKDS+PDDDyVJ0dHRV+3n4OCgu+++W9OnT9fPP/+syZMna8OGDdbbs4rzgHlRXH7bjmEYOnLkiM1qaNWrV1daWlqB914+K1Cc2oKDg3X27NkCtx8ePHjQur80BAcH6/DhwwVm40r7OJerV6+eDMNQSEiIoqKiCmy33357sccMDg7W0aNHCwSAy1elk0rvOmnatKleeuklbdmyRf/5z3905swZzZs376o1SlJiYmKBfYmJiWX2eRfF5dd6ZmamkpOT//Zaz87OVnJysk1baf45zP9MLq/vl19+KXRGzMfHR/3799fHH3+sU6dOqVmzZoWuyAeg8iIkAagUNmzYoFdeeUUhISHq06fPFfv99ttvBdryv5Q1KytLkuTu7i5JhYaWkli0aJFNUPnss8+UnJxsXVFN+usv/Nu3b7dZaeurr74qsJR1cWrr3LmzcnNz9eabb9q0z5gxQxaLxeb416Jz585KSUnRJ598Ym27dOmS5syZIw8PD911112lcpzLde/eXVWqVNHEiRMLhBrDMPTrr78We8zo6GidOXNGX375pbXt4sWLmj9/foG+7u7uRVqq+0oyMjJ06dIlm7amTZvKwcHBei0WplWrVqpVq5bmzZtn02/16tU6cOCAunTpUuKartW7776rnJwc6+u5c+fq0qVLBa71LVu2FHjf5TNJpfnnMCoqSk5OTpozZ47NtTJz5swCfS+/bjw8PFS/fv2r/kwAVD4sAQ7gurN69WodPHhQly5dUmpqqjZs2KB169YpODhYX375pc3D7Jd7+eWXtWXLFnXp0kXBwcE6d+6c3n77bdWuXVuRkZGS/vpLnLe3t+bNm6dq1arJ3d1dbdq0UUhISInq9fHxUWRkpPr376/U1FTNnDlT9evXt1kMYODAgfrss8907733qkePHjp69KgWL15cYMnm4tTWtWtXtW/fXi+++KJOnDihW2+9Vd98842++OILDR8+vETLQRdm0KBBeuedd9SvXz/t3r1bdevW1WeffaatW7dq5syZV31G7O8cOXJEkyZNKtDeokULdenSRZMmTVJsbKxOnDihbt26qVq1ajp+/LhWrFihQYMGafTo0cU63pNPPqk333xTvXv31rBhwxQQEKCPPvrIek2ZZzfCw8P1ySefaOTIkWrdurU8PDzUtWvXIh9rw4YNGjJkiB5++GHdcsstunTpkj788ENVqVJFMTExV3yfk5OTpkyZov79++uuu+5S7969rUuA161bVyNGjCjWOZem7Oxs3X333erRo4cSExP19ttvKzIy0mYhjIEDB+qpp55STEyMOnbsqH379mnt2rWqWbOmzVjNmzdXlSpVNGXKFKWnp8vFxUUdOnRQrVq1il2Xr6+vRo8erbi4ON13333q3Lmz9uzZo9WrVxc4bqNGjdSuXTuFh4fLx8dHu3bt0meffaYhQ4aU7EMBcH2yz6J6AFB8+cv65m/Ozs6Gv7+/0bFjR2PWrFk2S03nu3wJ8ISEBOOBBx4wAgMDDWdnZyMwMNDo3bu3cejQIZv3ffHFF0ajRo0MR0dHmyW377rrLqNx48aF1nelJcA//vhjIzY21qhVq5bh5uZmdOnSxTh58mSB90+bNs246aabDBcXF6Nt27bGrl27Cox5tdouXwLcMAzj/PnzxogRI4zAwEDDycnJCA0NNV5//XWbZZAN469lmQcPHlygpistTX651NRUo3///kbNmjUNZ2dno2nTpoUuU17cJcDNP2/zNmDAAGu/zz//3IiMjDTc3d0Nd3d3IywszBg8eLCRmJho7XOln1thn9mxY8eMLl26GG5uboavr68xatQo4/PPPzckGdu3b7f2y8zMNP75z38a3t7ehiTrOPk/98uX9j5+/LjNz+vYsWPG448/btSrV89wdXU1fHx8jPbt2xvr168v0ufzySefGC1atDBcXFwMHx8fo0+fPsbp06dt+pTGEuCF/bwuvy7z37t582Zj0KBBRvXq1Q0PDw+jT58+xq+//mrz3tzcXGPMmDFGzZo1japVqxrR0dHGkSNHCr3W5s+fb9x8881GlSpVirUceGHnkpuba0ycONEICAgw3NzcjHbt2hk//fRTgeNOmjTJuO222wxvb2/Dzc3NCAsLMyZPnmyztDmAys9iGBX0iVwAACqImTNnasSIETp9+rRuuukme5dT4eR/ue3OnTvVqlUre5cDANeMZ5IAADD5888/bV5fvHhR77zzjkJDQwlIAHCD4JkkAABMunfvrjp16qh58+ZKT0/X4sWLdfDgQX300Uf2Lu2Gl5mZqczMzKv28fX1veKy5QBQVIQkAABMoqOj9d577+mjjz5Sbm6uGjVqpKVLl6pnz572Lu2G98Ybb2jixIlX7XP8+HGbJccBoCR4JgkAAFwXjh07pmPHjl21T2Rk5FVXuASAoiAkAQAAAIAJCzcAAAAAgEmlfyYpLy9PZ8+eVbVq1Wy+BBAAAADAjcUwDJ0/f16BgYFycLjyfFGlD0lnz55VUFCQvcsAAAAAUEGcOnVKtWvXvuL+Sh+SqlWrJumvD8LT09PO1QAAAACwl4yMDAUFBVkzwpVU+pCUf4udp6cnIQkAAADA3z6Gw8INAAAAAGBCSAIAAAAAE0ISAAAAAJgQkgAAAADAhJAEAAAAACaEJAAAAAAwISQBAAAAgAkhCQAAAABMCEkAAAAAYEJIAgAAAAATQhIAAAAAmBCSAAAAAMCEkAQAAAAAJoQkAAAAADAhJAEAAACAiV1DUm5ursaOHauQkBC5ubmpXr16euWVV2QYhrWPYRgaN26cAgIC5ObmpqioKB0+fNiOVQMAAACozOwakqZMmaK5c+fqzTff1IEDBzRlyhRNnTpVc+bMsfaZOnWqZs+erXnz5mnHjh1yd3dXdHS0Ll68aMfKAQAAAFRWFsM8bVPO7rvvPvn5+en999+3tsXExMjNzU2LFy+WYRgKDAzUqFGjNHr0aElSenq6/Pz8tGDBAvXq1etvj5GRkSEvLy+lp6fL09OzzM4FAAAAQMVW1Gxg15mkO+64QwkJCTp06JAkad++ffr222/VqVMnSdLx48eVkpKiqKgo63u8vLzUpk0bbdu2rdAxs7KylJGRYbMBAAAAQFE52vPg//rXv5SRkaGwsDBVqVJFubm5mjx5svr06SNJSklJkST5+fnZvM/Pz8+673JxcXGaOHFi2RYOALjudO1q7wpsrVpl7woAAFdi15mkTz/9VB999JGWLFmi77//XgsXLtQbb7yhhQsXlnjM2NhYpaenW7dTp06VYsUAAAAAKju7ziQ999xz+te//mV9tqhp06Y6efKk4uLi1LdvX/n7+0uSUlNTFRAQYH1famqqmjdvXuiYLi4ucnFxKfPaAQAAAFROdp1J+uOPP+TgYFtClSpVlJeXJ0kKCQmRv7+/EhISrPszMjK0Y8cORURElGutAAAAAG4Mdp1J6tq1qyZPnqw6deqocePG2rNnj6ZPn67HH39ckmSxWDR8+HBNmjRJoaGhCgkJ0dixYxUYGKhu3brZs3QAAAAAlZRdQ9KcOXM0duxYPfPMMzp37pwCAwP15JNPaty4cdY+zz//vC5cuKBBgwYpLS1NkZGRWrNmjVxdXe1YOQAAAIDKyq7fk1Qe+J4kAIDE6nYAgOvke5IAAAAAoKIhJAEAAACACSEJAAAAAEwISQAAAABgQkgCAAAAABNCEgAAAACYEJIAAAAAwISQBAAAAAAmhCQAAAAAMCEkAQAAAIAJIQkAAAAATAhJAAAAAGBCSAIAAAAAE0ISAAAAAJgQkgAAAADAhJAEAAAAACaEJAAAAAAwISQBAAAAgAkhCQAAAABMCEkAAAAAYEJIAgAAAAATQhIAAAAAmBCSAAAAAMCEkAQAAAAAJoQkAAAAADAhJAEAAACACSEJAAAAAEwISQAAAABgQkgCAAAAABNCEgAAAACYEJIAAAAAwISQBAAAAAAmhCQAAAAAMCEkAQAAAIAJIQkAAAAATAhJAAAAAGBCSAIAAAAAE0ISAAAAAJgQkgAAAADAhJAEAAAAACaEJAAAAAAwISQBAAAAgAkhCQAAAABMCEkAAAAAYGLXkFS3bl1ZLJYC2+DBgyVJFy9e1ODBg1WjRg15eHgoJiZGqamp9iwZAAAAQCVn15C0c+dOJScnW7d169ZJkh5++GFJ0ogRI7Rq1SotW7ZMmzdv1tmzZ9W9e3d7lgwAAACgknO058F9fX1tXr/22muqV6+e7rrrLqWnp+v999/XkiVL1KFDB0lSfHy8GjZsqO3bt+v222+3R8kAAAAAKrkK80xSdna2Fi9erMcff1wWi0W7d+9WTk6OoqKirH3CwsJUp04dbdu27YrjZGVlKSMjw2YDAAAAgKKqMCFp5cqVSktLU79+/SRJKSkpcnZ2lre3t00/Pz8/paSkXHGcuLg4eXl5WbegoKAyrBoAAABAZVNhQtL777+vTp06KTAw8JrGiY2NVXp6unU7depUKVUIAAAA4EZg12eS8p08eVLr16/X8uXLrW3+/v7Kzs5WWlqazWxSamqq/P39rziWi4uLXFxcyrJcAAAAAJVYhZhJio+PV61atdSlSxdrW3h4uJycnJSQkGBtS0xMVFJSkiIiIuxRJgAAAIAbgN1nkvLy8hQfH6++ffvK0fH/y/Hy8tKAAQM0cuRI+fj4yNPTU88++6wiIiJY2Q4AAABAmbF7SFq/fr2SkpL0+OOPF9g3Y8YMOTg4KCYmRllZWYqOjtbbb79thyoBAAAA3CgshmEY9i6iLGVkZMjLy0vp6eny9PS0dzkAADvp2tXeFdhatcreFQDAjaeo2aBCPJMEAAAAABUFIQkAAAAATAhJAAAAAGBCSAIAAAAAE0ISAAAAAJgQkgAAAADAhJAEAAAAACaEJAAAAAAwISQBAAAAgAkhCQAAAABMCEkAAAAAYEJIAgAAAAATQhIAAAAAmBCSAAAAAMCEkAQAAAAAJoQkAAAAADAhJAEAAACACSEJAAAAAEwISQAAAABgQkgCAAAAABNCEgAAAACYEJIAAAAAwISQBAAAAAAmhCQAAAAAMCEkAQAAAIAJIQkAAAAATAhJAAAAAGBCSAIAAAAAE0ISAAAAAJgQkgAAAADAhJAEAAAAACaEJAAAAAAwISQBAAAAgAkhCQAAAABMCEkAAAAAYEJIAgAAAAATQhIAAAAAmBCSAAAAAMCEkAQAAAAAJoQkAAAAADAhJAEAAACACSEJAAAAAEwISQAAAABgQkgCAAAAABO7h6QzZ87okUceUY0aNeTm5qamTZtq165d1v2GYWjcuHEKCAiQm5uboqKidPjwYTtWDAAAAKAys2tI+v3339W2bVs5OTlp9erV+vnnnzVt2jRVr17d2mfq1KmaPXu25s2bpx07dsjd3V3R0dG6ePGiHSsHAAAAUFk52vPgU6ZMUVBQkOLj461tISEh1v82DEMzZ87USy+9pAceeECStGjRIvn5+WnlypXq1atXudcMAAAAoHKz60zSl19+qVatWunhhx9WrVq11KJFC82fP9+6//jx40pJSVFUVJS1zcvLS23atNG2bdsKHTMrK0sZGRk2GwAAAAAUlV1D0rFjxzR37lyFhoZq7dq1evrppzV06FAtXLhQkpSSkiJJ8vPzs3mfn5+fdd/l4uLi5OXlZd2CgoLK9iQAAAAAVCp2DUl5eXlq2bKlXn31VbVo0UKDBg3SE088oXnz5pV4zNjYWKWnp1u3U6dOlWLFAAAAACo7u4akgIAANWrUyKatYcOGSkpKkiT5+/tLklJTU236pKamWvddzsXFRZ6enjYbAAAAABSVXUNS27ZtlZiYaNN26NAhBQcHS/prEQd/f38lJCRY92dkZGjHjh2KiIgo11oBAAAA3BjsurrdiBEjdMcdd+jVV19Vjx499N133+ndd9/Vu+++K0myWCwaPny4Jk2apNDQUIWEhGjs2LEKDAxUt27d7Fk6AAAAgErKriGpdevWWrFihWJjY/Xyyy8rJCREM2fOVJ8+fax9nn/+eV24cEGDBg1SWlqaIiMjtWbNGrm6utqxcgAAAACVlcUwDMPeRZSljIwMeXl5KT09neeTAOAG1rWrvSuwtWqVvSsAgBtPUbOBXZ9JAgAAAICKhpAEAAAAACaEJAAAAAAwISQBAAAAgAkhCQAAAABMCEkAAAAAYEJIAgAAAAATQhIAAAAAmBCSAAAAAMCEkAQAAAAAJoQkAAAAADAhJAEAAACACSEJAAAAAEwISQAAAABgQkgCAAAAABNCEgAAAACYEJIAAAAAwISQBAAAAAAmhCQAAAAAMCEkAQAAAIAJIQkAAAAATAhJAAAAAGBCSAIAAAAAE0ISAAAAAJgQkgAAAADAhJAEAAAAACaEJAAAAAAwISQBAAAAgAkhCQAAAABMCEkAAAAAYEJIAgAAAAATQhIAAAAAmBCSAAAAAMCEkAQAAAAAJoQkAAAAADAhJAEAAACACSEJAAAAAEwISQAAAABgQkgCAAAAABNCEgAAAACYEJIAAAAAwISQBAAAAAAmhCQAAAAAMCEkAQAAAICJXUPShAkTZLFYbLawsDDr/osXL2rw4MGqUaOGPDw8FBMTo9TUVDtWDAAAAKCys/tMUuPGjZWcnGzdvv32W+u+ESNGaNWqVVq2bJk2b96ss2fPqnv37nasFgAAAEBl52j3Ahwd5e/vX6A9PT1d77//vpYsWaIOHTpIkuLj49WwYUNt375dt99+e3mXCgAAAOAGYPeZpMOHDyswMFA333yz+vTpo6SkJEnS7t27lZOTo6ioKGvfsLAw1alTR9u2bbvieFlZWcrIyLDZAAAAAKCo7BqS2rRpowULFmjNmjWaO3eujh8/rn/84x86f/68UlJS5OzsLG9vb5v3+Pn5KSUl5YpjxsXFycvLy7oFBQWV8VkAAAAAqEzsertdp06drP/drFkztWnTRsHBwfr000/l5uZWojFjY2M1cuRI6+uMjAyCEgAAAIAis/vtdmbe3t665ZZbdOTIEfn7+ys7O1tpaWk2fVJTUwt9himfi4uLPD09bTYAAAAAKKoKFZIyMzN19OhRBQQEKDw8XE5OTkpISLDuT0xMVFJSkiIiIuxYJQAAAIDKzK63240ePVpdu3ZVcHCwzp49q/Hjx6tKlSrq3bu3vLy8NGDAAI0cOVI+Pj7y9PTUs88+q4iICFa2AwAAAFBm7BqSTp8+rd69e+vXX3+Vr6+vIiMjtX37dvn6+kqSZsyYIQcHB8XExCgrK0vR0dF6++237VkyAAAAgErOYhiGYe8iylJGRoa8vLyUnp7O80kAcAPr2tXeFdhatcreFQDAjaeo2aBCPZMEAAAAAPZGSAIAAAAAE0ISAAAAAJgQkgAAAADApEQh6dixY6VdBwAAAABUCCUKSfXr11f79u21ePFiXbx4sbRrAgAAAAC7KVFI+v7779WsWTONHDlS/v7+evLJJ/Xdd9+Vdm0AAAAAUO5KFJKaN2+uWbNm6ezZs/rggw+UnJysyMhINWnSRNOnT9cvv/xS2nUCAAAAQLm4poUbHB0d1b17dy1btkxTpkzRkSNHNHr0aAUFBemxxx5TcnJyadUJAAAAAOXimkLSrl279MwzzyggIEDTp0/X6NGjdfToUa1bt05nz57VAw88UFp1AgAAAEC5cCzJm6ZPn674+HglJiaqc+fOWrRokTp37iwHh78yV0hIiBYsWKC6deuWZq0AAAAAUOZKFJLmzp2rxx9/XP369VNAQEChfWrVqqX333//mooDAAAAgPJWopB0+PDhv+3j7Oysvn37lmR4AAAAALCbEj2TFB8fr2XLlhVoX7ZsmRYuXHjNRQEAAACAvZQoJMXFxalmzZoF2mvVqqVXX331mosCAAAAAHspUUhKSkpSSEhIgfbg4GAlJSVdc1EAAAAAYC8lCkm1atXSDz/8UKB93759qlGjxjUXBQAAAAD2UqKQ1Lt3bw0dOlQbN25Ubm6ucnNztWHDBg0bNky9evUq7RoBAAAAoNyUaHW7V155RSdOnNDdd98tR8e/hsjLy9Njjz3GM0kAAAAArmslCknOzs765JNP9Morr2jfvn1yc3NT06ZNFRwcXNr1AQAAAEC5KlFIynfLLbfolltuKa1aAAAAAMDuShSScnNztWDBAiUkJOjcuXPKy8uz2b9hw4ZSKQ4AAAAAyluJQtKwYcO0YMECdenSRU2aNJHFYintugAAAADALkoUkpYuXapPP/1UnTt3Lu16AAAAAMCuSrQEuLOzs+rXr1/atQAAAACA3ZUoJI0aNUqzZs2SYRilXQ8AAAAA2FWJbrf79ttvtXHjRq1evVqNGzeWk5OTzf7ly5eXSnEAAAAAUN5KFJK8vb314IMPlnYtAAAAAGB3JQpJ8fHxpV0HAAAAAFQIJXomSZIuXbqk9evX65133tH58+clSWfPnlVmZmapFQcAAAAA5a1EM0knT57Uvffeq6SkJGVlZaljx46qVq2apkyZoqysLM2bN6+06wQAAACAclGimaRhw4apVatW+v333+Xm5mZtf/DBB5WQkFBqxQEAAABAeSvRTNJ//vMf/fe//5Wzs7NNe926dXXmzJlSKQwAAAAA7KFEM0l5eXnKzc0t0H769GlVq1btmosCAAAAAHspUUi65557NHPmTOtri8WizMxMjR8/Xp07dy6t2gAAAACg3JXodrtp06YpOjpajRo10sWLF/XPf/5Thw8fVs2aNfXxxx+Xdo0AAAAAUG5KFJJq166tffv2aenSpfrhhx+UmZmpAQMGqE+fPjYLOQAAAADA9aZEIUmSHB0d9cgjj5RmLQAAAABgdyUKSYsWLbrq/scee6xExQAAAACAvZUoJA0bNszmdU5Ojv744w85OzuratWqhCQAAAAA160SrW73+++/22yZmZlKTExUZGQkCzcAAAAAuK6VKCQVJjQ0VK+99lqBWSYAAAAAuJ6UWkiS/lrM4ezZs6U5JAAAAACUqxI9k/Tll1/avDYMQ8nJyXrzzTfVtm3bUikMAAAAAOyhRDNJ3bp1s9m6d++uCRMmqFmzZvrggw9KVMhrr70mi8Wi4cOHW9suXryowYMHq0aNGvLw8FBMTIxSU1NLND4AAAAAFEWJZpLy8vJKtYidO3fqnXfeUbNmzWzaR4wYoX//+99atmyZvLy8NGTIEHXv3l1bt24t1eMDAAAAQL5SfSapJDIzM9WnTx/Nnz9f1atXt7anp6fr/fff1/Tp09WhQweFh4crPj5e//3vf7V9+3Y7VgwAAACgMivRTNLIkSOL3Hf69OlX3T948GB16dJFUVFRmjRpkrV99+7dysnJUVRUlLUtLCxMderU0bZt23T77bcXOl5WVpaysrKsrzMyMopcKwAAAACUKCTt2bNHe/bsUU5Ojho0aCBJOnTokKpUqaKWLVta+1kslquOs3TpUn3//ffauXNngX0pKSlydnaWt7e3Tbufn59SUlKuOGZcXJwmTpxYjLMBAAAAgP9XopDUtWtXVatWTQsXLrTeIvf777+rf//++sc//qFRo0b97RinTp3SsGHDtG7dOrm6upakjELFxsbazHRlZGQoKCio1MYHAAAAULmV6JmkadOmKS4uzuYZourVq2vSpEmaNm1akcbYvXu3zp07p5YtW8rR0VGOjo7avHmzZs+eLUdHR/n5+Sk7O1tpaWk270tNTZW/v/8Vx3VxcZGnp6fNBgAAAABFVaKZpIyMDP3yyy8F2n/55RedP3++SGPcfffd+vHHH23a+vfvr7CwMI0ZM0ZBQUFycnJSQkKCYmJiJEmJiYlKSkpSREREScoGAAAAgL9VopD04IMPqn///po2bZpuu+02SdKOHTv03HPPqXv37kUao1q1amrSpIlNm7u7u2rUqGFtHzBggEaOHCkfHx95enrq2WefVURExBUXbQAAAACAa1WikDRv3jyNHj1a//znP5WTk/PXQI6OGjBggF5//fVSK27GjBlycHBQTEyMsrKyFB0drbfffrvUxgcAAACAy1kMwzBK+uYLFy7o6NGjkqR69erJ3d291AorLRkZGfLy8lJ6ejrPJwHADaxrV3tXYGvVKntXAAA3nqJmg2v6Mtnk5GQlJycrNDRU7u7uuoa8BQAAAAAVQolC0q+//qq7775bt9xyizp37qzk5GRJfz1DVJTlvwEAAACgoipRSBoxYoScnJyUlJSkqlWrWtt79uypNWvWlFpxAAAAAFDeSrRwwzfffKO1a9eqdu3aNu2hoaE6efJkqRQGAAAAAPZQopmkCxcu2Mwg5fvtt9/k4uJyzUUBAAAAgL2UKCT94x//0KJFi6yvLRaL8vLyNHXqVLVv377UigMAAACA8lai2+2mTp2qu+++W7t27VJ2draef/557d+/X7/99pu2bt1a2jUCAAAAQLkp0UxSkyZNdOjQIUVGRuqBBx7QhQsX1L17d+3Zs0f16tUr7RoBAAAAoNwUeyYpJydH9957r+bNm6cXX3yxLGoCAAAAALsp9kySk5OTfvjhh7KoBQAAAADsrkS32z3yyCN6//33S7sWAAAAALC7Ei3ccOnSJX3wwQdav369wsPD5e7ubrN/+vTppVIcAAAAAJS3YoWkY8eOqW7duvrpp5/UsmVLSdKhQ4ds+lgsltKrDgAAAADKWbFCUmhoqJKTk7Vx40ZJUs+ePTV79mz5+fmVSXEAAAAAUN6K9UySYRg2r1evXq0LFy6UakEAAAAAYE8lWrgh3+WhCQAAAACud8UKSRaLpcAzRzyDBAAAAKAyKdYzSYZhqF+/fnJxcZEkXbx4UU899VSB1e2WL19eehUCAAAAQDkqVkjq27evzetHHnmkVIsBAAAAAHsrVkiKj48vqzoAAAAAoEK4poUbAAAAAKCyISQBAAAAgAkhCQAAAABMCEkAAAAAYEJIAgAAAAATQhIAAAAAmBCSAAAAAMCEkAQAAAAAJoQkAAAAADAhJAEAAACACSEJAAAAAEwISQAAAABgQkgCAAAAABNCEgAAAACYEJIAAAAAwISQBAAAAAAmhCQAAAAAMCEkAQAAAIAJIQkAAAAATAhJAAAAAGBCSAIAAAAAE0ISAAAAAJgQkgAAAADAhJAEAAAAACZ2DUlz585Vs2bN5OnpKU9PT0VERGj16tXW/RcvXtTgwYNVo0YNeXh4KCYmRqmpqXasGAAAAEBlZ9eQVLt2bb322mvavXu3du3apQ4dOuiBBx7Q/v37JUkjRozQqlWrtGzZMm3evFlnz55V9+7d7VkyAAAAgErOYhiGYe8izHx8fPT666/roYcekq+vr5YsWaKHHnpIknTw4EE1bNhQ27Zt0+23316k8TIyMuTl5aX09HR5enqWZekAgAqsa1d7V2Br1Sp7VwAAN56iZoMK80xSbm6uli5dqgsXLigiIkK7d+9WTk6OoqKirH3CwsJUp04dbdu27YrjZGVlKSMjw2YDAAAAgKKye0j68ccf5eHhIRcXFz311FNasWKFGjVqpJSUFDk7O8vb29umv5+fn1JSUq44XlxcnLy8vKxbUFBQGZ8BAAAAgMrE7iGpQYMG2rt3r3bs2KGnn35affv21c8//1zi8WJjY5Wenm7dTp06VYrVAgAAAKjsHO1dgLOzs+rXry9JCg8P186dOzVr1iz17NlT2dnZSktLs5lNSk1Nlb+//xXHc3FxkYuLS1mXDQAAAKCSsvtM0uXy8vKUlZWl8PBwOTk5KSEhwbovMTFRSUlJioiIsGOFAAAAACozu84kxcbGqlOnTqpTp47Onz+vJUuWaNOmTVq7dq28vLw0YMAAjRw5Uj4+PvL09NSzzz6riIiIIq9sBwAAAADFZdeQdO7cOT322GNKTk6Wl5eXmjVrprVr16pjx46SpBkzZsjBwUExMTHKyspSdHS03n77bXuWDAAAAKCSq3Dfk1Ta+J4kAIDE9yQBAK7D70kCAAAAgIqAkAQAAAAAJoQkAAAAADAhJAEAAACACSEJAAAAAEwISQAAAABgQkgCAAAAABNCEgAAAACYEJIAAAAAwISQBAAAAAAmhCQAAAAAMCEkAQAAAIAJIQkAAAAATAhJAAAAAGBCSAIAAAAAE0ISAAAAAJgQkgAAAADAhJAEAAAAACaEJAAAAAAwISQBAAAAgAkhCQAAAABMCEkAAAAAYEJIAgAAAAATQhIAAAAAmBCSAAAAAMCEkAQAAAAAJoQkAAAAADAhJAEAAACACSEJAAAAAEwISQAAAABgQkgCAAAAABNCEgAAAACYEJIAAAAAwISQBAAAAAAmhCQAAAAAMCEkAQAAAIAJIQkAAAAATAhJAAAAAGBCSAIAAAAAE0ISAAAAAJgQkgAAAADAhJAEAAAAACaEJAAAAAAwISQBAAAAgIldQ1JcXJxat26tatWqqVatWurWrZsSExNt+ly8eFGDBw9WjRo15OHhoZiYGKWmptqpYgAAAACVnV1D0ubNmzV48GBt375d69atU05Oju655x5duHDB2mfEiBFatWqVli1bps2bN+vs2bPq3r27HasGAAAAUJlZDMMw7F1Evl9++UW1atXS5s2bdeeddyo9PV2+vr5asmSJHnroIUnSwYMH1bBhQ23btk233377346ZkZEhLy8vpaeny9PTs6xPAQBQQXXtau8KbK1aZe8KAODGU9RsUKGeSUpPT5ck+fj4SJJ2796tnJwcRUVFWfuEhYWpTp062rZtW6FjZGVlKSMjw2YDAAAAgKKqMCEpLy9Pw4cPV9u2bdWkSRNJUkpKipydneXt7W3T18/PTykpKYWOExcXJy8vL+sWFBRU1qUDAAAAqEQqTEgaPHiwfvrpJy1duvSaxomNjVV6erp1O3XqVClVCAAAAOBG4GjvAiRpyJAh+uqrr7RlyxbVrl3b2u7v76/s7GylpaXZzCalpqbK39+/0LFcXFzk4uJS1iUDAAAAqKTsOpNkGIaGDBmiFStWaMOGDQoJCbHZHx4eLicnJyUkJFjbEhMTlZSUpIiIiPIuFwAAAMANwK4zSYMHD9aSJUv0xRdfqFq1atbnjLy8vOTm5iYvLy8NGDBAI0eOlI+Pjzw9PfXss88qIiKiSCvbAQAAAEBx2TUkzZ07V5LUrl07m/b4+Hj169dPkjRjxgw5ODgoJiZGWVlZio6O1ttvv13OlQIAAAC4UVSo70kqC3xPEgBA4nuSAADX6fckAQAAAIC9EZIAAAAAwISQBAAAAAAmhCQAAAAAMCEkAQAAAIAJIQkAAAAATAhJAAAAAGBCSAIAAAAAE0ISAAAAAJgQkgAAAADAhJAEAAAAACaEJAAAAAAwISQBAAAAgAkhCQAAAABMCEkAAAAAYEJIAgAAAAATQhIAAAAAmBCSAAAAAMCEkAQAAAAAJoQkAAAAADAhJAEAAACACSEJAAAAAEwISQAAAABgQkgCAAAAABNCEgAAAACYEJIAAAAAwISQBAAAAAAmhCQAAAAAMCEkAQAAAIAJIQkAAAAATAhJAAAAAGBCSAIAAAAAE0ISAAAAAJgQkgAAAADAhJAEAAAAACaEJAAAAAAwISQBAAAAgAkhCQAAAABMCEkAAAAAYEJIAgAAAAATQhIAAAAAmBCSAAAAAMCEkAQAAAAAJoQkAAAAADCxa0jasmWLunbtqsDAQFksFq1cudJmv2EYGjdunAICAuTm5qaoqCgdPnzYPsUCAAAAuCHYNSRduHBBt956q956661C90+dOlWzZ8/WvHnztGPHDrm7uys6OloXL14s50oBAAAA3Cgc7XnwTp06qVOnToXuMwxDM2fO1EsvvaQHHnhAkrRo0SL5+flp5cqV6tWrV3mWCgAAAOAGUWGfSTp+/LhSUlIUFRVlbfPy8lKbNm20bdu2K74vKytLGRkZNhsAAAAAFFWFDUkpKSmSJD8/P5t2Pz8/677CxMXFycvLy7oFBQWVaZ0AAAAAKpcKG5JKKjY2Vunp6dbt1KlT9i4JAAAAwHWkwoYkf39/SVJqaqpNe2pqqnVfYVxcXOTp6WmzAQAAAEBRVdiQFBISIn9/fyUkJFjbMjIytGPHDkVERNixMgAAAACVmV1Xt8vMzNSRI0esr48fP669e/fKx8dHderU0fDhwzVp0iSFhoYqJCREY8eOVWBgoLp162a/ogEAAABUanYNSbt27VL79u2tr0eOHClJ6tu3rxYsWKDnn39eFy5c0KBBg5SWlqbIyEitWbNGrq6u9ioZAAAAQCVnMQzDsHcRZSkjI0NeXl5KT0/n+SQAuIF17WrvCmytWmXvCgDgxlPUbFBhn0kCAAAAAHsgJAEAAACACSEJAAAAAEwISQAAAABgQkgCAAAAABNCEgAAAACYEJIAAAAAwISQBAAAAAAmhCQAAAAAMCEkAQAAAIAJIQkAAAAATAhJAAAAAGBCSAIAAAAAE0ISAAAAAJgQkgAAAADAhJAEAAAAACaEJAAAAAAwISQBAAAAgAkhCQAAAABMCEkAAAAAYEJIAgAAAAATQhIAAAAAmBCSAAAAAMCEkAQAAAAAJoQkAAAAADAhJAEAAACACSEJAAAAAEwISQAAAABgQkgCAAAAABNCEgAAAACYEJIAAAAAwISQBAAAAAAmhCQAAAAAMCEkAQAAAIAJIQkAAAAATAhJAAAAAGBCSAIAAAAAE0ISAAAAAJgQkgAAAADAhJAEAAAAACaEJAAAAAAwISQBAAAAgAkhCQAAAABMCEkAAAAAYHJdhKS33npLdevWlaurq9q0aaPvvvvO3iUBAAAAqKQqfEj65JNPNHLkSI0fP17ff/+9br31VkVHR+vcuXP2Lg0AAABAJVThQ9L06dP1xBNPqH///mrUqJHmzZunqlWr6oMPPrB3aQAAAAAqIUd7F3A12dnZ2r17t2JjY61tDg4OioqK0rZt2wp9T1ZWlrKysqyv09PTJUkZGRllWywAoELLybF3Bbb4vyUAKH/5mcAwjKv2q9Ah6X//+59yc3Pl5+dn0+7n56eDBw8W+p64uDhNnDixQHtQUFCZ1AgAQEl4edm7AgC4cZ0/f15eV/lFXKFDUknExsZq5MiR1td5eXn67bffVKNGDVksFjtWhqvJyMhQUFCQTp06JU9PT3uXgwqO6wXFxTWD4uKaQXFxzVwfDMPQ+fPnFRgYeNV+FTok1axZU1WqVFFqaqpNe2pqqvz9/Qt9j4uLi1xcXGzavL29y6pElDJPT09+saDIuF5QXFwzKC6uGRQX10zFd7UZpHwVeuEGZ2dnhYeHKyEhwdqWl5enhIQERURE2LEyAAAAAJVVhZ5JkqSRI0eqb9++atWqlW677TbNnDlTFy5cUP/+/e1dGgAAAIBKqMKHpJ49e+qXX37RuHHjlJKSoubNm2vNmjUFFnPA9c3FxUXjx48vcKskUBiuFxQX1wyKi2sGxcU1U7lYjL9b/w4AAAAAbiAV+pkkAAAAAChvhCQAAAAAMCEkAQAAAIAJIQkAAAAATAhJuCZbtmxR165dFRgYKIvFopUrV9rsT01NVb9+/RQYGKiqVavq3nvv1eHDh236HD16VA8++KB8fX3l6empHj16FPgC4cKcOXNGjzzyiGrUqCE3Nzc1bdpUu3btKs3TQxmw1zWTm5ursWPHKiQkRG5ubqpXr55eeeUVsXZNxRcXF6fWrVurWrVqqlWrlrp166bExESbPhcvXtTgwYNVo0YNeXh4KCYmpsA1kZSUpC5duqhq1aqqVauWnnvuOV26dOmqx/7tt9/Up08feXp6ytvbWwMGDFBmZmapnyNKl72umRMnTmjAgAE2v2fGjx+v7OzsMjlPlB57/p7Jl5WVpebNm8tisWjv3r2ldWooIUISrsmFCxd066236q233iqwzzAMdevWTceOHdMXX3yhPXv2KDg4WFFRUbpw4YL1/ffcc48sFos2bNigrVu3Kjs7W127dlVeXt4Vj/v777+rbdu2cnJy0urVq/Xzzz9r2rRpql69epmdK0qHva6ZKVOmaO7cuXrzzTd14MABTZkyRVOnTtWcOXPK7FxROjZv3qzBgwdr+/btWrdunXJycnTPPfdYrwlJGjFihFatWqVly5Zp8+bNOnv2rLp3727dn5ubqy5duig7O1v//e9/tXDhQi1YsEDjxo276rH79Omj/fv3a926dfrqq6+0ZcsWDRo0qMzOFaXDXtfMwYMHlZeXp3feeUf79+/XjBkzNG/ePL3wwgtler64dvb8PZPv+eefV2BgYKmfG0rIAEqJJGPFihXW14mJiYYk46effrK25ebmGr6+vsb8+fMNwzCMtWvXGg4ODkZ6erq1T1pammGxWIx169Zd8VhjxowxIiMjS/8kUK7K85rp0qWL8fjjj9u0de/e3ejTp08pnQ3Ky7lz5wxJxubNmw3D+Ovn7+TkZCxbtsza58CBA4YkY9u2bYZhGMbXX39tODg4GCkpKdY+c+fONTw9PY2srKxCj/Pzzz8bkoydO3da21avXm1YLBbjzJkzZXFqKCPldc0UZurUqUZISEgpnQnKS3lfM19//bURFhZm7N+/35Bk7Nmzp/RPCsXCTBLKTFZWliTJ1dXV2ubg4CAXFxd9++231j4Wi8Xmi9dcXV3l4OBg7VOYL7/8Uq1atdLDDz+sWrVqqUWLFpo/f34ZnQnKS1leM3fccYcSEhJ06NAhSdK+ffv07bffqlOnTmVxKihD6enpkiQfHx9J0u7du5WTk6OoqChrn7CwMNWpU0fbtm2TJG3btk1Nmza1+SLy6OhoZWRkaP/+/YUeZ9u2bfL29larVq2sbVFRUXJwcNCOHTtK/bxQdsrrmrnSsfOPi+tHeV4zqampeuKJJ/Thhx+qatWqZXE6KAFCEspM/i+P2NhY/f7778rOztaUKVN0+vRpJScnS5Juv/12ubu7a8yYMfrjjz904cIFjR49Wrm5udY+hTl27Jjmzp2r0NBQrV27Vk8//bSGDh2qhQsXltfpoQyU5TXzr3/9S7169VJYWJicnJzUokULDR8+XH369Cmv00MpyMvL0/Dhw9W2bVs1adJEkpSSkiJnZ2d5e3vb9PXz81NKSoq1j/kvLvn78/cVJiUlRbVq1bJpc3R0lI+PzxXfg4qnPK+Zyx05ckRz5szRk08+eY1ngfJUnteMYRjq16+fnnrqKZt/kIH9EZJQZpycnLR8+XIdOnRIPj4+qlq1qjZu3KhOnTrJweGvS8/X11fLli3TqlWr5OHhIS8vL6Wlpally5bWPoXJy8tTy5Yt9eqrr6pFixYaNGiQnnjiCc2bN6+8Tg9loCyvmU8//VQfffSRlixZou+//14LFy7UG2+8QbC+zgwePFg//fSTli5dau9ScJ2w1zVz5swZ3XvvvXr44Yf1xBNPlOuxcW3K85qZM2eOzp8/r9jY2DI/ForH0d4FoHILDw/X3r17lZ6eruzsbPn6+qpNmzY2/1pyzz336OjRo/rf//4nR0dHeXt7y9/fXzfffPMVxw0ICFCjRo1s2ho2bKjPP/+8zM4F5aOsrpnnnnvOOpskSU2bNtXJkycVFxenvn37lvl54doNGTLEunhC7dq1re3+/v7Kzs5WWlqazb/ypqamyt/f39rnu+++sxkvf1Wq/D6X8/f317lz52zaLl26pN9+++2K70HFUt7XTL6zZ8+qffv2uuOOO/Tuu++W0tmgPJT3NbNhwwZt27bN5hZySWrVqpX69OnDP+TZETNJKBdeXl7y9fXV4cOHtWvXLj3wwAMF+tSsWVPe3t7asGGDzp07p/vvv/+K47Vt27bA0pyHDh1ScHBwqdcO+yjta+aPP/4oMNNUpUqVq66Ih4rBMAwNGTJEK1as0IYNGxQSEmKzPzw8XE5OTkpISLC2JSYmKikpSREREZKkiIgI/fjjjzahZ926dfL09CzwDy75IiIilJaWpt27d1vbNmzYoLy8PLVp06Y0TxGlzF7XjPTXDFK7du0UHh6u+Pj4q85wo+Kw1zUze/Zs7du3T3v37tXevXv19ddfS5I++eQTTZ48ubRPE8Vh33UjcL07f/68sWfPHmPPnj2GJGP69OnGnj17jJMnTxqGYRiffvqpsXHjRuPo0aPGypUrjeDgYKN79+42Y3zwwQfGtm3bjCNHjhgffvih4ePjY4wcOdKmT4cOHYw5c+ZYX3/33XeGo6OjMXnyZOPw4cPGRx99ZFStWtVYvHhx2Z80rom9rpm+ffsaN910k/HVV18Zx48fN5YvX27UrFnTeP7558v+pHFNnn76acPLy8vYtGmTkZycbN3++OMPa5+nnnrKqFOnjrFhwwZj165dRkREhBEREWHdf+nSJaNJkybGPffcY+zdu9dYs2aN4evra8TGxlr77Nixw2jQoIFx+vRpa9u9995rtGjRwtixY4fx7bffGqGhoUbv3r3L58RRYva6Zk6fPm3Ur1/fuPvuu43Tp0/bHBsVmz1/z5gdP36c1e0qCEISrsnGjRsNSQW2vn37GoZhGLNmzTJq165tODk5GXXq1DFeeumlAstgjhkzxvDz8zOcnJyM0NBQY9q0aUZeXp5Nn+DgYGP8+PE2batWrTKaNGliuLi4GGFhYca7775blqeKUmKvayYjI8MYNmyYUadOHcPV1dW4+eabjRdffLFYS/nCPgq7XiQZ8fHx1j5//vmn8cwzzxjVq1c3qlatajz44IMF/mJ64sQJo1OnToabm5tRs2ZNY9SoUUZOTo51f/61efz4cWvbr7/+avTu3dvw8PAwPD09jf79+xvnz58v61PGNbLXNRMfH3/FY6Nis+fvGTNCUsVhMQy+bh4AAAAA8nGjLAAAAACYEJIAAAAAwISQBAAAAAAmhCQAAAAAMCEkAQAAAIAJIQkAAAAATAhJAAAAAGBCSAIAAAAAE0ISAMCu+vXrp27dupX6uCkpKerYsaPc3d3l7e1drscuC3Xr1tXMmTOv2sdisWjlypXlUg8AVGaEJAC4AVSEMHDixAlZLBbt3bu3XI43Y8YMJScna+/evTp06FChfWbNmqUFCxaUSz1mCxYsuGJwu5KdO3dq0KBBZVMQAMCGo70LAACgLBw9elTh4eEKDQ29Yh8vL69yrOja+Pr62rsEALhhMJMEANBPP/2kTp06ycPDQ35+fnr00Uf1v//9z7q/Xbt2Gjp0qJ5//nn5+PjI399fEyZMsBnj4MGDioyMlKurqxo1aqT169fb3P4VEhIiSWrRooUsFovatWtn8/433nhDAQEBqlGjhgYPHqycnJyr1jx37lzVq1dPzs7OatCggT788EPrvrp16+rzzz/XokWLZLFY1K9fv0LHuHyGrSjnabFYNHfuXHXq1Elubm66+eab9dlnn1n3b9q0SRaLRWlpada2vXv3ymKx6MSJE9q0aZP69++v9PR0WSwWWSyWAscozOW32x0+fFh33nmn9fNet26dTf/s7GwNGTJEAQEBcnV1VXBwsOLi4v72OAAAQhIA3PDS0tLUoUMHtWjRQrt27dKaNWuUmpqqHj162PRbuHCh3N3dtWPHDk2dOlUvv/yy9S/mubm56tatm6pWraodO3bo3Xff1Ysvvmjz/u+++06StH79eiUnJ2v58uXWfRs3btTRo0e1ceNGLVy4UAsWLLjqbXArVqzQsGHDNGrUKP3000968skn1b9/f23cuFHSX7em3XvvverRo4eSk5M1a9asIn8eVzvPfGPHjlVMTIz27dunPn36qFevXjpw4ECRxr/jjjs0c+ZMeXp6Kjk5WcnJyRo9enSR65OkvLw8de/eXc7OztqxY4fmzZunMWPG2PSZPXu2vvzyS3366adKTEzURx99pLp16xbrOABwo+J2OwC4wb355ptq0aKFXn31VWvbBx98oKCgIB06dEi33HKLJKlZs2YaP368JCk0NFRvvvmmEhIS1LFjR61bt05Hjx7Vpk2b5O/vL0maPHmyOnbsaB0z/3axGjVqWPvkq169ut58801VqVJFYWFh6tKlixISEvTEE08UWvMbb7yhfv366ZlnnpEkjRw5Utu3b9cbb7yh9u3by9fXVy4uLnJzcytwrL9ztfPM9/DDD2vgwIGSpFdeeUXr1q3TnDlz9Pbbb//t+M7OzvLy8pLFYil2bfnWr1+vgwcPau3atQoMDJQkvfrqq+rUqZO1T1JSkkJDQxUZGSmLxaLg4OASHQsAbkTMJAHADW7fvn3auHGjPDw8rFtYWJikv57rydesWTOb9wUEBOjcuXOSpMTERAUFBdn8pf+2224rcg2NGzdWlSpVCh27MAcOHFDbtm1t2tq2bVvk2Zyrudp55ouIiCjwujSOXVQHDhxQUFCQNSAVVlO/fv20d+9eNWjQQEOHDtU333xTbvUBwPWOmSQAuMFlZmaqa9eumjJlSoF9AQEB1v92cnKy2WexWJSXl1cqNZTl2OVdi4PDX//+aBiGte3vnq8qCy1bttTx48e1evVqrV+/Xj169FBUVJTN81MAgMIxkwQAN7iWLVtq//79qlu3rurXr2+zubu7F2mMBg0a6NSpU0pNTbW27dy506aPs7OzpL+eX7pWDRs21NatW23atm7dqkaNGl3z2EWxffv2Aq8bNmwo6f9vK0xOTrbuv3zZc2dn52v6HBo2bKhTp07ZHOPymiTJ09NTPXv21Pz58/XJJ5/o888/12+//Vbi4wLAjYKZJAC4QaSnpxf4y3r+SnLz589X7969rau6HTlyREuXLtV7771ncxvclXTs2FH16tVT3759NXXqVJ0/f14vvfSSpL9mYiSpVq1acnNz05o1a1S7dm25urqWeAnu5557Tj169FCLFi0UFRWlVatWafny5Vq/fn2JxiuuZcuWqVWrVoqMjNRHH32k7777Tu+//74kqX79+goKCtKECRM0efJkHTp0SNOmTbN5f926dZWZmamEhATdeuutqlq1qqpWrVrk40dFRemWW25R37599frrrysjI6PAQhnTp09XQECAWrRoIQcHBy1btkz+/v7F/n4mALgRMZMEADeITZs2qUWLFjbbxIkTFRgYqK1btyo3N1f33HOPmjZtquHDh8vb29t669jfqVKlilauXKnMzEy1bt1aAwcOtP6l3dXVVZLk6Oio2bNn65133lFgYKAeeOCBEp9Lt27dNGvWLL3xxhtq3Lix3nnnHcXHxxdYVrysTJw4UUuXLlWzZs20aNEiffzxx9ZZLCcnJ3388cc6ePCgmjVrpilTpmjSpEk277/jjjv01FNPqWfPnvL19dXUqVOLdXwHBwetWLFCf/75p2677TYNHDhQkydPtulTrVo1TZ06Va1atVLr1q114sQJff3110X+mQLAjcximG+aBgCglGzdulWRkZE6cuSI6tWrZ+9ySo3FYtGKFStsvl8JAFC5cLsdAKBUrFixQh4eHgoNDdWRI0c0bNgwtW3btlIFJADAjYGQBAAoFefPn9eYMWOUlJSkmjVrKioqqsCzOCjcf/7zH5vvOLpcZmZmOVYDAOB2OwAA7OzPP//UmTNnrri/fv365VgNAICQBAAAAAAmLHEDAAAAACaEJAAAAAAwISQBAAAAgAkhCQAAAABMCEkAAAAAYEJIAgAAAAATQhIAAAAAmPwfWTzabpR7alUAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jP3R4enP3m19"
   },
   "source": [
    "### How does the base model do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vxbl4ACsyRgi"
   },
   "source": [
    "Optionally, you can check how Mistral does on one of your data samples. For example, if you have a dataset of users' biometric data to their health scores, you could test the following `eval_prompt`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KRhfq_Fa3m19"
   },
   "source": [
    "The `eval_prompt` I used was:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "eval_prompts = [\n",
    "    'CS Alumni event',\n",
    "    'Write a promotional post about the Computer Science Alumni event',\n",
    "    'Queer Trivia at Cat in the Cream',\n",
    "    'Boba with International Student and Scholar Service (ISSS) staff members',\n",
    "    'SOSHA Craft Night',\n",
    "    'Pizza at 1PM Monday',\n",
    "    'Pizza at King Building',\n",
    "    'Professor Beer at the \\'Sco by Economics Department',\n",
    "    'Pizza at King Building, 1PM on Monday'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file = 'automated_outputs_base_' + dataset_name + '.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "NidIuFXMyRgi",
    "outputId": "b1794b11-9a22-4b0a-e871-7df039ab59fc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CS Alumni event:\n",
      "\n",
      "The Department of Computer Science and Engineering, IIT Kharagpur is organizing a one day alumni meet on 16th December 2017. The objective of the meet is to bring together all the alumni from CSE department and provide them an opportunity to interact with each other as well as with the faculty members. This will also be an occasion for the alumni to share their experiences in industry/academia and inspire the current students.\n",
      "\n",
      "Venue: Seminar Hall, CSE Building, IIT Kharagpur\n",
      "Date: Saturday, 16th December 2017\n",
      "Time: 9 AM - 5 PM\n",
      "\n",
      "Registration link: https://goo.gl/forms/834jqYKZJXkfQvgx1\n",
      "\n",
      "For more details contact:\n",
      "Dr. Sourav Chakraborty (sourav@cse.iitkgp.ernet.in)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a promotional post about the Computer Science Alumni event on 10/26.\n",
      "\n",
      "The Computer Science Department is hosting an alumni event on October 26th, from 5:30-7:30pm in the CSE building. The event will feature a panel of alumni who are working at local companies and startups. They will discuss their career paths since graduating from UW, as well as what they do now. There will also be time for networking with other alumni and current students. This is a great opportunity to learn more about careers in computer science and meet people who have been successful in this field.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queer Trivia at Cat in the Cream\n",
      "\n",
      "The Cat in the Cream is a queer-friendly bar and restaurant located on 1307 N. High St., Columbus, OH 43201. The bar has been around since 1985 and was named after the cat that used to live there. Itâ€™s a great place for drinks, food, and fun!\n",
      "\n",
      "Queer trivia night is every Wednesday from 6:30pm â€“ 8:30pm. There are prizes for first, second, and third place teams. You can also win prizes by answering bonus questions throughout the game.\n",
      "\n",
      "If you want to play, just show up with your team of four or less people (or join one when you get there). If you donâ€™t have a team, they will help you find one.\n",
      "\n",
      "There is no cover charge but tips are appreciated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boba with International Student and Scholar Service (ISSS) staff members\n",
      "\n",
      "Boba is a popular Taiwanese drink that has become increasingly popular in the United States. Itâ€™s made of tapioca balls, milk tea, and sugar. The tapioca balls are chewy and have a unique texture. They can be eaten by themselves or added to other drinks like coffee or smoothies.\n",
      "\n",
      "The first time I tried boba was when my friend brought me some from her hometown in China. She said it was very common there but not so much here in America. When she told me about how delicious they were, I couldnâ€™t wait to try them myself! So we went out together one night after class and got ourselves some bubble tea at our favorite spot near campus. We both loved it right away â€“ especially since ours came with extra-large straws so we could suck up all those little pearls without having any trouble getting them down our throats ðŸ™‚\n",
      "\n",
      "## What Is Boba?\n",
      "\n",
      "Boba is a type of tea that originated in Taiwan. It consists of small balls made from tapioca starch, which are then cooked until they become soft and chewy. These balls are usually served with milk or fruit juice as well as ice cream on top.\n",
      "\n",
      "There\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOSHA Craft Night\n",
      "\n",
      "SOSHA is hosting a craft night on Thursday, March 28th from 6:30-9pm at the SOSHA office. We will be making cards for our residents and their families to celebrate Easter! This event is open to all volunteers and staff members. Please RSVP by emailing sosha@soshashelter.org or calling (517) 485-8486.\n",
      "\n",
      "Volunteer Training\n",
      "\n",
      "We are holding volunteer training sessions on Tuesday, April 2nd and Wednesday, April 3rd from 6:30-8:30pm at the SOSHA office. If you would like to attend one of these trainings please contact us at sosha@soshashelter.org or call (517) 485-8486.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pizza at 1PM Monday?\n",
      "\n",
      "Iâ€™m not sure if I should be proud of myself or ashamed. It was a good pizza, but it wasnâ€™t the best pizza in town. And I had to walk past two other places that serve better pizza on my way home from work. But I didnâ€™t care. I wanted pizza and I got it.\n",
      "\n",
      "It was a nice day out so I decided to eat outside. The place is called â€œPizzeria Unoâ€ and they have a patio with tables and chairs. There were only three people there when I arrived. One guy eating alone, one couple sitting together, and me.\n",
      "\n",
      "The menu has all kinds of pizzas listed by number. They also offer salads, sandwiches, and desserts. I ordered #20 which is a pepperoni pizza with mushrooms and green peppers. It came with a side salad.\n",
      "\n",
      "When I asked for extra cheese, she said no problem. She then added some more sauce to the top of the pie before putting it into the oven. After about ten minutes, she brought it back out and cut it up into eight slices. Then she put it onto a plate and handed it over to me.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pizza at King Building\n",
      "\n",
      "The King Building is a 1920s-era building in the heart of downtown. It was originally built as an office building, but has been converted into apartments and retail space. The pizza place on the ground floor is called â€œPizzaâ€ (I know, I knowâ€¦). They have a nice selection of beers on tap, including some local brews from places like New Belgium and Odell. Their pizzas are pretty good too!\n",
      "\n",
      "## What is the best pizza in Fort Collins?\n",
      "\n",
      "There are many great pizza places in Fort Collins, but here are our top picks:\n",
      "\n",
      "1. Pizzeria Locale â€“ This restaurant serves up delicious Neapolitan style pies with fresh ingredients that will make your mouth water. You canâ€™t go wrong with any of their options!\n",
      "2. Pizza Casbah â€“ If youâ€™re looking for something different than traditional Italian fare then this spot should be on your list. They offer unique flavors such as lamb sausage or even vegan cheese if thatâ€™s what floats your boat!\n",
      "3. Pieology â€“ For those who want to build their own personalized pie thereâ€™s no better place than Pieology where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Professor Beer at the 'Sco by Economics Department\n",
      "\n",
      "The Economics Department hosted a talk on Thursday, April 12th with Professor Robert Beer from the University of California, Santa Barbara. The event was held in the â€˜Sco and was attended by students and faculty members alike.\n",
      "\n",
      "Professor Beerâ€™s research focuses on the economics of education, labor markets, and public policy. He has published articles in leading academic journals such as the American Economic Review, Journal of Political Economy, Quarterly Journal of Economics, and Journal of Labor Economics. His current work examines the effects of school choice programs on student achievement, teacher quality, and segregation; the impact of college financial aid policies on educational attainment; and the role of social networks in shaping career outcomes.\n",
      "\n",
      "During his visit to UC Berkeley, Professor Beer gave an informal presentation about his research on the effects of school choice programs on student achievement. In particular, he discussed how these programs can help improve educational outcomes for low-income students.\n",
      "\n",
      "After the presentation, there was a Q&A session where students had the opportunity to ask questions about Professor Beerâ€™s research and get advice on pursuing careers in academia or consulting. Overall, it was\n",
      "Pizza at King Building, 1PM on Monday\n",
      "\n",
      "The next meeting of the UW-Madison chapter of the American Society for Biochemistry and Molecular Biology (ASBMB) will be held in room 205 of the King Building on Monday, November 3rd from 1:00 to 2:00 PM. The speaker is Dr. David G. Nelson, Professor of Chemistry and Biomolecular Chemistry at UW-Madison. His talk is entitled â€œChemical Biology of Protein Folding.â€\n",
      "\n",
      "Abstract:\n",
      "Proteins are essential components of all living organisms. They carry out a wide range of functions including catalysis, transport, signaling, structural support, and regulation. In order to perform these diverse roles, proteins must fold into their native structures with high fidelity. This process is often referred to as protein folding. However, this term can be misleading because it implies that there is only one structure that a given polypeptide chain can adopt. In fact, many proteins have multiple stable conformations, some of which may be biologically relevant while others may not. Furthermore, proteins do not always fold correctly. For example, mutations or other perturbations can\n"
     ]
    }
   ],
   "source": [
    "# Init an eval tokenizer that doesn't add padding or eos token\n",
    "eval_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    add_bos_token=True,\n",
    ")\n",
    "for eval_prompt in eval_prompts:\n",
    "    model_input = eval_tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        result = eval_tokenizer.decode(model.generate(**model_input, max_new_tokens=256, repetition_penalty=1.15)[0], skip_special_tokens=True)\n",
    "        now = datetime.now()\n",
    "        with open(result_file, 'a') as file:\n",
    "            file.write('- time: ' + str(now)+ '\\n')\n",
    "            file.write('- formating: ' + formatting + '\\n')\n",
    "            file.write('- input: ' + eval_prompt + '\\n')\n",
    "            file.write('- output: ' + '\\n' + result + '\\n')\n",
    "            file.write('-------------------- \\n')\n",
    "        print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dCAWeCzZyRgi"
   },
   "source": [
    "Observe how the model does out of the box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AapDoyfAyRgi"
   },
   "source": [
    "### 4. Set Up LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mp2gMi1ZzGET"
   },
   "source": [
    "Now, to start our fine-tuning, we have to apply some preprocessing to the model to prepare it for training. For that use the `prepare_model_for_kbit_training` method from PEFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "a9EUEDAl0ss3"
   },
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "gkIcwsSU01EB"
   },
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cUYEpEK-yRgj"
   },
   "source": [
    "Let's print the model to examine its layers, as we will apply QLoRA to all the linear layers of the model. Those layers are `q_proj`, `k_proj`, `v_proj`, `o_proj`, `gate_proj`, `up_proj`, `down_proj`, and `lm_head`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "XshGNsbxyRgj",
    "outputId": "c619b0e8-8516-4d4b-9abe-13eaa3f3b204",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm()\n",
      "        (post_attention_layernorm): MistralRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I6mTLuQJyRgj"
   },
   "source": [
    "Here we define the LoRA config.\n",
    "\n",
    "`r` is the rank of the low-rank matrix used in the adapters, which thus controls the number of parameters trained. A higher rank will allow for more expressivity, but there is a compute tradeoff.\n",
    "\n",
    "`alpha` is the scaling factor for the learned weights. The weight matrix is scaled by `alpha/r`, and thus a higher value for `alpha` assigns more weight to the LoRA activations.\n",
    "\n",
    "The values used in the QLoRA paper were `r=64` and `lora_alpha=16`, and these are said to generalize well, but we will use `r=32` and `lora_alpha=64` so that we have more emphasis on the new fine-tuned data while also reducing computational complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Ybeyl20n3dYH",
    "outputId": "6a16c182-04d9-4812-ae81-502a8fe364d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 85041152 || all params: 3837112320 || trainable%: 2.2162799758751914\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,  # Conventional\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X_FHi_VLyRgn"
   },
   "source": [
    "See how the model looks different now, with the LoRA adapters added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "IaYMWak4yRgn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): MistralForCausalLM(\n",
      "      (model): MistralModel(\n",
      "        (embed_tokens): Embedding(32000, 4096)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x MistralDecoderLayer(\n",
      "            (self_attn): MistralSdpaAttention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): MistralRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): MistralMLP(\n",
      "              (gate_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=14336, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=14336, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=14336, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): MistralRMSNorm()\n",
      "            (post_attention_layernorm): MistralRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): MistralRMSNorm()\n",
      "      )\n",
      "      (lm_head): lora.Linear(\n",
      "        (base_layer): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "        (lora_dropout): ModuleDict(\n",
      "          (default): Dropout(p=0.05, inplace=False)\n",
      "        )\n",
      "        (lora_A): ModuleDict(\n",
      "          (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "        )\n",
      "        (lora_B): ModuleDict(\n",
      "          (default): Linear(in_features=32, out_features=32000, bias=False)\n",
      "        )\n",
      "        (lora_embedding_A): ParameterDict()\n",
      "        (lora_embedding_B): ParameterDict()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_0MOtwf3zdZp"
   },
   "source": [
    "### 5. Run Training!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fEe0uWYSyRgo"
   },
   "source": [
    "I didn't have a lot of training samples: only about 200 total train/validation. I used 500 training steps, and I was fine with overfitting in this case. I found that the end product worked well. It took about 20 minutes on the 1x A10G 24GB.\n",
    "\n",
    "Overfitting is when the validation loss goes up (bad) while the training loss goes down significantly, meaning the model is learning the training set really well, but is unable to generalize to new datapoints. In most cases, this is not desired, but since I am just playing around with a model to generate outputs like my journal entries, I was fine with a moderate amount of overfitting.\n",
    "\n",
    "With that said, a note on training: you can set the `max_steps` to be high initially, and examine at what step your model's performance starts to degrade. There is where you'll find a sweet spot for how many steps to perform. For example, say you start with 1000 steps, and find that at around 500 steps the model starts overfitting, as described above. Therefore, 500 steps would be your sweet spot, so you would use the `checkpoint-500` model repo in your output dir (`mistral-journal-finetune`) as your final model in step 6 below.\n",
    "\n",
    "If you're just doing something for fun like I did and are OK with overfitting, you can try different checkpoint versions with different degrees of overfitting.\n",
    "\n",
    "You can interrupt the process via Kernel -> Interrupt Kernel in the top nav bar once you realize you didn't need to train anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "c_L1131GyRgo"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 1: # If more than 1 GPU\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0216588d55814fd29b1e7ed8ddf0b33a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/users/quota/students/2020/ymai/miniconda3/envs/yap/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4636, 'grad_norm': 6.691591739654541, 'learning_rate': 2.3797595190380762e-05, 'epoch': 0.83}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5354a5294aa04f8b8fb059398c8ebfc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8544031381607056, 'eval_runtime': 2.0406, 'eval_samples_per_second': 9.801, 'eval_steps_per_second': 1.47, 'epoch': 0.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/users/quota/students/2020/ymai/miniconda3/envs/yap/lib/python3.11/site-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/usr/users/quota/students/2020/ymai/miniconda3/envs/yap/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.494, 'grad_norm': 4.963532447814941, 'learning_rate': 2.2545090180360722e-05, 'epoch': 1.67}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57e8357cd0904efd8624a72182c942d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6598242521286011, 'eval_runtime': 2.0421, 'eval_samples_per_second': 9.794, 'eval_steps_per_second': 1.469, 'epoch': 1.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/users/quota/students/2020/ymai/miniconda3/envs/yap/lib/python3.11/site-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/usr/users/quota/students/2020/ymai/miniconda3/envs/yap/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3213, 'grad_norm': 4.879208087921143, 'learning_rate': 2.1292585170340683e-05, 'epoch': 2.5}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff68700927174f109ff55898fea979df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6631893515586853, 'eval_runtime': 2.043, 'eval_samples_per_second': 9.79, 'eval_steps_per_second': 1.468, 'epoch': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/users/quota/students/2020/ymai/miniconda3/envs/yap/lib/python3.11/site-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/usr/users/quota/students/2020/ymai/miniconda3/envs/yap/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1948, 'grad_norm': 4.268137454986572, 'learning_rate': 2.0040080160320643e-05, 'epoch': 3.33}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "135be117906e41d7951c667f8c40bc7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7783767580986023, 'eval_runtime': 2.042, 'eval_samples_per_second': 9.795, 'eval_steps_per_second': 1.469, 'epoch': 3.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/users/quota/students/2020/ymai/miniconda3/envs/yap/lib/python3.11/site-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/usr/users/quota/students/2020/ymai/miniconda3/envs/yap/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1527, 'grad_norm': 2.887470006942749, 'learning_rate': 1.87875751503006e-05, 'epoch': 4.17}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7598a5b786f440ca1b45f1093421dd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6491894125938416, 'eval_runtime': 2.0418, 'eval_samples_per_second': 9.795, 'eval_steps_per_second': 1.469, 'epoch': 4.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/users/quota/students/2020/ymai/miniconda3/envs/yap/lib/python3.11/site-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/usr/users/quota/students/2020/ymai/miniconda3/envs/yap/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1231, 'grad_norm': 1.7320647239685059, 'learning_rate': 1.7535070140280564e-05, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de5a21b90b0a4ef499249464c4ec1c2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7473962903022766, 'eval_runtime': 2.0407, 'eval_samples_per_second': 9.8, 'eval_steps_per_second': 1.47, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/users/quota/students/2020/ymai/miniconda3/envs/yap/lib/python3.11/site-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/usr/users/quota/students/2020/ymai/miniconda3/envs/yap/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0864, 'grad_norm': 3.6205060482025146, 'learning_rate': 1.628256513026052e-05, 'epoch': 5.83}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf590e00c6864abfb431e2bce689a848",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7960249185562134, 'eval_runtime': 2.0407, 'eval_samples_per_second': 9.801, 'eval_steps_per_second': 1.47, 'epoch': 5.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/users/quota/students/2020/ymai/miniconda3/envs/yap/lib/python3.11/site-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/usr/users/quota/students/2020/ymai/miniconda3/envs/yap/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0873, 'grad_norm': 1.793352484703064, 'learning_rate': 1.5030060120240483e-05, 'epoch': 6.67}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13a088fc1ccc48d2af0cbce4bae20681",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7840228080749512, 'eval_runtime': 2.0428, 'eval_samples_per_second': 9.791, 'eval_steps_per_second': 1.469, 'epoch': 6.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/users/quota/students/2020/ymai/miniconda3/envs/yap/lib/python3.11/site-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/usr/users/quota/students/2020/ymai/miniconda3/envs/yap/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0776, 'grad_norm': 3.336414098739624, 'learning_rate': 1.3777555110220442e-05, 'epoch': 7.5}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d4afac260ba4bb4868ca9abad1a2e59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.802385687828064, 'eval_runtime': 2.0425, 'eval_samples_per_second': 9.792, 'eval_steps_per_second': 1.469, 'epoch': 7.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/users/quota/students/2020/ymai/miniconda3/envs/yap/lib/python3.11/site-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/usr/users/quota/students/2020/ymai/miniconda3/envs/yap/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0706, 'grad_norm': 3.7004098892211914, 'learning_rate': 1.25250501002004e-05, 'epoch': 8.33}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70a0d31321974cf29f1797fd50391f3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8570483922958374, 'eval_runtime': 2.0423, 'eval_samples_per_second': 9.793, 'eval_steps_per_second': 1.469, 'epoch': 8.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/users/quota/students/2020/ymai/miniconda3/envs/yap/lib/python3.11/site-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/usr/users/quota/students/2020/ymai/miniconda3/envs/yap/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0653, 'grad_norm': 2.06253981590271, 'learning_rate': 1.1272545090180361e-05, 'epoch': 9.17}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a56f39db4eee4c678710bed8343f44a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9078471064567566, 'eval_runtime': 2.0421, 'eval_samples_per_second': 9.794, 'eval_steps_per_second': 1.469, 'epoch': 9.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/users/quota/students/2020/ymai/miniconda3/envs/yap/lib/python3.11/site-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/usr/users/quota/students/2020/ymai/miniconda3/envs/yap/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.06, 'grad_norm': 5.782634258270264, 'learning_rate': 1.0020040080160322e-05, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0462771f0c114fdea41c8e26828a37a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9248605966567993, 'eval_runtime': 2.0429, 'eval_samples_per_second': 9.79, 'eval_steps_per_second': 1.468, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/users/quota/students/2020/ymai/miniconda3/envs/yap/lib/python3.11/site-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/usr/users/quota/students/2020/ymai/miniconda3/envs/yap/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0575, 'grad_norm': 2.2902944087982178, 'learning_rate': 8.767535070140282e-06, 'epoch': 10.83}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11332c526d1e49468cdbb89927fb4a09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.939368724822998, 'eval_runtime': 2.0431, 'eval_samples_per_second': 9.789, 'eval_steps_per_second': 1.468, 'epoch': 10.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/users/quota/students/2020/ymai/miniconda3/envs/yap/lib/python3.11/site-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/usr/users/quota/students/2020/ymai/miniconda3/envs/yap/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0587, 'grad_norm': 2.1884231567382812, 'learning_rate': 7.515030060120242e-06, 'epoch': 11.67}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "214cedc56c994bb49ea4a5df0e1c847f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9393275380134583, 'eval_runtime': 2.042, 'eval_samples_per_second': 9.794, 'eval_steps_per_second': 1.469, 'epoch': 11.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/users/quota/students/2020/ymai/miniconda3/envs/yap/lib/python3.11/site-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/usr/users/quota/students/2020/ymai/miniconda3/envs/yap/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0563, 'grad_norm': 1.7294750213623047, 'learning_rate': 6.2625250501002e-06, 'epoch': 12.5}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45318b8a4eff43658d53e7a8ad160d90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9626482725143433, 'eval_runtime': 2.0404, 'eval_samples_per_second': 9.802, 'eval_steps_per_second': 1.47, 'epoch': 12.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/users/quota/students/2020/ymai/miniconda3/envs/yap/lib/python3.11/site-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/usr/users/quota/students/2020/ymai/miniconda3/envs/yap/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0571, 'grad_norm': 1.8209202289581299, 'learning_rate': 5.010020040080161e-06, 'epoch': 13.33}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b353215cf9bf4ee2876589cd7ed79074",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9752551317214966, 'eval_runtime': 2.0406, 'eval_samples_per_second': 9.801, 'eval_steps_per_second': 1.47, 'epoch': 13.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/users/quota/students/2020/ymai/miniconda3/envs/yap/lib/python3.11/site-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/usr/users/quota/students/2020/ymai/miniconda3/envs/yap/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.055, 'grad_norm': 1.7397435903549194, 'learning_rate': 3.757515030060121e-06, 'epoch': 14.17}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dffc962457b2482eacb9883e09771a96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9808386564254761, 'eval_runtime': 2.0396, 'eval_samples_per_second': 9.806, 'eval_steps_per_second': 1.471, 'epoch': 14.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/users/quota/students/2020/ymai/miniconda3/envs/yap/lib/python3.11/site-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/usr/users/quota/students/2020/ymai/miniconda3/envs/yap/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0556, 'grad_norm': 1.8878099918365479, 'learning_rate': 2.5050100200400804e-06, 'epoch': 15.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "718b42a2a4f945939c5ce07ecc76389d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9889930486679077, 'eval_runtime': 2.0417, 'eval_samples_per_second': 9.796, 'eval_steps_per_second': 1.469, 'epoch': 15.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/users/quota/students/2020/ymai/miniconda3/envs/yap/lib/python3.11/site-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/usr/users/quota/students/2020/ymai/miniconda3/envs/yap/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0539, 'grad_norm': 2.1062533855438232, 'learning_rate': 1.2525050100200402e-06, 'epoch': 15.83}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b61ab9b6cc1e4abfbfc127a5c19f28d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9917470216751099, 'eval_runtime': 2.0397, 'eval_samples_per_second': 9.805, 'eval_steps_per_second': 1.471, 'epoch': 15.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/users/quota/students/2020/ymai/miniconda3/envs/yap/lib/python3.11/site-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/usr/users/quota/students/2020/ymai/miniconda3/envs/yap/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0533, 'grad_norm': 2.1047768592834473, 'learning_rate': 0.0, 'epoch': 16.67}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80c13a185dfd48d586b92e456bc4bac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9927123188972473, 'eval_runtime': 2.0407, 'eval_samples_per_second': 9.801, 'eval_steps_per_second': 1.47, 'epoch': 16.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/users/quota/students/2020/ymai/miniconda3/envs/yap/lib/python3.11/site-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1003.7675, 'train_samples_per_second': 0.996, 'train_steps_per_second': 0.498, 'train_loss': 0.18220321345329285, 'epoch': 16.67}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=500, training_loss=0.18220321345329285, metrics={'train_runtime': 1003.7675, 'train_samples_per_second': 0.996, 'train_steps_per_second': 0.498, 'total_flos': 8634841497600000.0, 'train_loss': 0.18220321345329285, 'epoch': 16.666666666666668})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "from datetime import datetime\n",
    "\n",
    "project = \"journal-finetune\"\n",
    "base_model_name = \"mistral\"\n",
    "run_name = base_model_name + \"-\" + project\n",
    "output_dir = \"./\" + run_name\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        warmup_steps=1,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=1,\n",
    "        gradient_checkpointing=True,\n",
    "        max_steps=500,\n",
    "        learning_rate=2.5e-5,  # Want a small lr for finetuning\n",
    "        bf16=True,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        logging_steps=25,              # When to start reporting loss\n",
    "        logging_dir=\"./logs\",          # Directory for storing logs\n",
    "        save_strategy=\"steps\",         # Save the model checkpoint every logging step\n",
    "        save_steps=25,                 # Save checkpoints every 50 steps\n",
    "        evaluation_strategy=\"steps\",   # Evaluate the model every logging step\n",
    "        eval_steps=25,                 # Evaluate and save checkpoints every 50 steps\n",
    "        do_eval=True,                  # Perform evaluation at the end of training\n",
    "        report_to=\"none\"               # Ensure no external reporting\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R9rRmDCeQiTJ"
   },
   "source": [
    "I cleared the output of the cell above because I stopped the training early, and it produced a long, ugly error message."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0D57XqcsyRgo"
   },
   "source": [
    "### 6. Drum Roll... Try the Trained Model!\n",
    "\n",
    "It's a good idea to kill the current process so that you don't run out of memory loading the base model again on top of the model we just trained. Go to `Kernel > Restart Kernel` or kill the process via the Terminal (`nvidia smi` > `kill [PID]`). \n",
    "\n",
    "By default, the PEFT library will only save the QLoRA adapters, so we need to first load the base model from the Huggingface Hub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['load_in_8bit_fp32_cpu_offload']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ec90152792e43a79c7c7bc5a018cf6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "base_model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    load_in_8bit_fp32_cpu_offload=True  # Enable CPU offloading for parts of the model\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    # device_map=\"auto\",  # You can replace \"auto\" with a custom map if needed\n",
    "    device_map=\"cuda\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "eval_tokenizer = AutoTokenizer.from_pretrained(base_model_id, add_bos_token=True, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_BxOhAiqyRgp"
   },
   "source": [
    "Now load the QLoRA adapter from the appropriate checkpoint directory, i.e. the best performing model checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "GwsiqhWuyRgp"
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "ft_model = PeftModel.from_pretrained(base_model, \"mistral-journal-finetune/checkpoint-300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lX39ibolyRgp"
   },
   "source": [
    "and run your inference!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UUehsaVNyRgp"
   },
   "source": [
    "Let's try the same `eval_prompt` and thus `model_input` as above, and see if the new finetuned model performs better. I like playing with the repetition penalty (just little tweaks of .01-.05 at a time). THIS IS SO FUN. I'm obsessed wth this AI version of myself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'advertisement'\n",
    "result_file = 'automated_outputs_finetuned_' + dataset_name + '.txt'\n",
    "formatting = \"### Input: {example['input']} ### Output: {example['output']}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "eval_prompts = [\n",
    "    'CS Alumni event',\n",
    "    'Write a promotional post about the Computer Science Alumni event',\n",
    "    'Queer Trivia at Cat in the Cream',\n",
    "    'Boba with International Student and Scholar Service (ISSS) staff members',\n",
    "    'SOSHA Craft Night',\n",
    "    'Pizza at 1PM Monday',\n",
    "    'Pizza at King Building',\n",
    "    'Professor Beer at the \\'Sco by Economics Department',\n",
    "    'Pizza at King Building, 1PM on Monday'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "lMkVNEUvyRgp",
    "outputId": "7d49d409-5dbe-4306-c1a4-9d87e3073397"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CS Alumni event: AI and Machine Learning\n",
      "\n",
      "Date: 20th March, 2019 (Wednesday)\n",
      "Time: 6.30pm - 8.30pm\n",
      "Venue: LT-4, Level 5, EEE Building, The Chinese University of Hong Kong, Shatin, N.T., HK\n",
      "Fee: Free admission. Register now!\n",
      "\n",
      "Speaker: Prof. Wefu Tsco (CUHK Computer Science alumnus), CUHK Computer Science alumni\n",
      "\n",
      "Abstract: Artificial intelligence (psychology) is the study of how to make computers do things that require intelligence when done by humans. It is related to the similar task of using computers to understand human intelligence, but AI does not have to confine itself to methods that are biologically observable or psychologically credible; it can draw on mathematical modeling, computer science, and engineering ingenuity to develop intelligent behavior in oblivious systems. Machine learning is a subfield of artificial intelligence concerned with the design and development of algorithms that allow machines to learn from data and improve their performance over time. ML algorithms are used for tasks such as image recognition, natural language processing, and\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a promotional post about the Computer Science Alumni event.\n",
      "\n",
      "ðŸŒŸðŸ’»ðŸŽ“ðŸŒŸ Computer Science Alumni Event Tickets for CS Enthusiasts! ðŸŒŸðŸ’»ðŸŽ“ðŸŒŸ\n",
      "\n",
      "Calling all computer science alumni and enthusiasts! ðŸŒŸðŸ’»ðŸŽ“ðŸŒŸ Join us for a night of networking and celebration with a touch of computer science alumni pride! ðŸŒŸðŸ”ðŸš€\n",
      "\n",
      "ðŸŒŸðŸ‘©â€ðŸ’»ðŸŒŸðŸ«ðŸŒŸðŸŒŸðŸ•°ï¸ðŸŒŸðŸ§ ðŸŒŸðŸŒŸðŸŒŸðŸ–¥ï¸ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queer Trivia at Cat in the Cream!\n",
      "ðŸŒˆðŸŒŸ Tickets for our next trivia night are now available! ðŸŒŸðŸ³ï¸â€ðŸŒˆðŸ»ðŸŒŸðŸ“šðŸŒˆðŸŒŸðŸ‘©â€ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boba with International Student and Scholar Service (ISSS) staff members.\n",
      "\n",
      "Boba is a popular drink among international students at the University of Oregon, and itâ€™s not hard to see why! This delicious beverage combines chewy tapioca pearls with sweet syrup and milk or tea for a refreshing and satisfying treat. For many international students, boba is morethan just a drink; itâ€™s a cultural experience that connects them to their home countries and creates a sense of community with other international students on campus. Whether enjoying boba with friends or sipping on a cup while studying, this beloved drink brings a touch of international flavor to life in Eugene! ðŸµðŸŒðŸŒŸ International student and scholar service (ISSS) staff members love bonding over boba with their international student clients. It's a great way to connect over shared culture and enjoy a delicious treat together! ðŸŒŸðŸ¥›ðŸ‘©â€ðŸŽ“ðŸŒ #internationalstudentlife #bobatea #culturalconnection #ISSS #UpsideDownWorld #InternationalStudentAndScholarService #GlobalCommunity #TasteTheWorld #BobaLove ðŸŒŸðŸ½\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOSHA Craft Night\n",
      "\n",
      "Spleodar, the Science and Technology Festival for young people will be taking place in Cscientific and technological festival for young people. The festival is aimed at primary school children and their families and aims to inspire a love of science and technology through interactive exhibits, workshops, and demonstrations. Tickets are available online and the event promises to be a fun-filled day of discovery and exploration!\n",
      "\n",
      "ðŸ”¬ðŸš€ Spleodar Science and Technology Festival tickets for young people's scientific and technological festival. Perfect for young minds eager to explore the world of STEM with a touch of Spleodar science and technology festival adventure! ðŸŒŸðŸ§ªðŸŒŒðŸ¤–ðŸŒŸðŸ‘©â€ðŸ”¬ðŸŒŸðŸ¦ ðŸŒŸðŸ•°ï¸ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pizza at 1PM Monday, then a trip to the zscaler office for some networking.\n",
      "\n",
      "Iâ€™ve been meaning to get out and see more of the city since moving here in January. Iâ€™m not sure why it took me so long to do this, but better late then never!\n",
      "\n",
      "The pizza was delicious and the company was great. We chatted about all sorts of things from work to life outside the office. It was nice to catch up with everyone over a slice or two (or three).\n",
      "\n",
      "After lunch we headed off to the zscaler office where we met up with some colleagues for some networking fun. The atmosphere was buzzing as people mingled and caught up on whatâ€™s new in their lives.\n",
      "\n",
      "Overall it was an enjoyable day filled with good food, friendly faces, and plenty of conversation. Looking forward to doing it again soon! ðŸ•ðŸŒŸ Networking with friends at zscaler office #pizzaparty #networkingfun #friendshipgoals\n",
      "\n",
      "## Exploring the City: A Day of Sightseeing and Cultural Immersion\n",
      "\n",
      "Explore the city with a touch of sightseeing and cultural immersion! ï¿½ï¿½ï¿½ï¿½ï¿½\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pizza at King Building\n",
      "\n",
      "# Pscyhic Development Seminar with John Edward\n",
      "\n",
      "Join us for a night of psychic development and exploration with renowned medium John Edward! Perfect for those interested in the paranormal and unlocking their psychic potential with a touch of John Edward's spiritual wisdom. ðŸŒŸðŸ”®ðŸŒŸ Tickets for psychic enthusiasts. Perfect for those seeking psychic enlightenment with a touch of John Edward's mediumistic magic! ðŸŒŸðŸ”®ðŸ“ƒðŸŒŸðŸ‘»ðŸŒŸðŸ•¯ï¸ðŸŒŸ tickets for psychic seminars with a touch of John Edward's spiritual guidance! ðŸŒŸðŸ”®ðŸ§ ðŸŒŸðŸ‘¼ðŸŒŸðŸŒŸðŸŒˆðŸŒŸ tickets for psychic development with a touch of John Edward's mediumship mastery! ðŸŒŸðŸ”®ðŸŒŸðŸ”ðŸŒŸðŸŒŸðŸŒŸðŸŒ±ðŸŒŸ tickets for psychic exploration with a touch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Professor Beer at the 'Sco by Economics Department\n",
      "\n",
      "The economics department is hosting a beer tasting event on Friday, April 26 from 5:30-7 p.m. in the â€˜scscscscoâ€™ to celebrate the end of the semester and raise money for the economics club! Tickets are $10 each and can be purchased here. The event will feature a variety of craft beers for attendees to sample and enjoy while learning about beer tasting with a touch of economics professor humor! Don't miss out on this fun and educational event with a touch of beer tasting and economics professor humor - perfect for beer lovers and exploring the world of craft beer with a touch of economics professor wit and wisdom! ðŸºðŸŒŸðŸ“ˆðŸ¦ðŸŒŸðŸ»ðŸŒŸðŸŽ“ðŸŒŸðŸºðŸŒŸðŸ‘©â€ðŸ’¼ðŸŒŸðŸ·ðŸŒŸðŸ”¬ðŸŒŸðŸºðŸŒŸðŸ§ªðŸŒŸðŸºðŸŒŸðŸ½ï¸ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½\n",
      "Pizza at King Building, 1PM on Monday.\n",
      "\n",
      "## Agenda\n",
      "\n",
      "- Discuss the future of the group and how to best support each other in our career development journeys!\n",
      "- Connect with fellow career enthusiasts and gain insights into career growth with a touch of career development group empowerment! ðŸŒŸðŸ‘©â€ðŸ’¼ Career Development Group meeting tickets for career enthusiasts. Perfect for those on a path to career success and gaining inspiration from career development group support! ðŸŒŸðŸ“šðŸŒŸï¸ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½\n"
     ]
    }
   ],
   "source": [
    "for eval_prompt in eval_prompts:\n",
    "    model_input = eval_tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    ft_model.eval()\n",
    "    with torch.no_grad():\n",
    "        result = eval_tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=256, repetition_penalty=1.15)[0], skip_special_tokens=True)\n",
    "        with open(result_file, 'a') as file:\n",
    "            now = datetime.now()\n",
    "            file.write('- time: ' + str(now) + '\\n')\n",
    "            file.write('- formating: ' + formatting + '\\n')\n",
    "            file.write('- input: ' + eval_prompt + '\\n')\n",
    "            file.write('- output: ' + '\\n' + result + '\\n')\n",
    "            file.write('-------------------- \\n')\n",
    "        print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VCJnpZoayRgq"
   },
   "source": [
    "### Sweet... it worked! The fine-tuned model now prints out journal entries in my style!\n",
    "\n",
    "How funny to see it write like me as an angsty teenager, and honestly adult. I am obsessed. It knows who my friends are and talks about them, and covers the same topics I usually cover. It's really cool.\n",
    "\n",
    "That output is quite private but I wanted you to see an example run, so I tweaked the `eval_prompt` so that it explicitly wouldn't say anything too sensitive, haha.\n",
    "\n",
    "I hope you enjoyed this tutorial on fine-tuning Mistral on your own data. If you have any questions, feel free to reach out to us on [X](https://x.com/brevdev) or [Discord](https://discord.gg/RN2a436M73).\n",
    "\n",
    "ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
